{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EISr4m0S2ydx"
      },
      "source": [
        "Kaggle/Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "bNLpE7z2487f",
        "outputId": "2a579b2d-98a1-485b-ff6f-96c9224dcdbc"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2481267056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install kagglehub --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    501\u001b[0m             make_files(\n\u001b[1;32m    502\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[1;32m    859\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \"\"\"\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install kagglehub --quiet\n",
        "import kagglehub\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRd5QLqR25tZ"
      },
      "source": [
        "Import and store dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvMZGaBL5B5n"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive (if not already done)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install Kaggle CLI if missing\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Setup Kaggle API credentials (make sure kaggle.json is in your Drive)\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and unzip dataset locally\n",
        "!kaggle datasets download -d tanjemahamed/mental-fatigue-level-detection-fatigueset-data --unzip -p /content/fatigue_data\n",
        "\n",
        "# List local downloaded files to verify\n",
        "!ls /content/fatigue_data\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "source_dir = '/content/fatigue_data/fatigueset'  # The actual dataset folder\n",
        "drive_dest = '/content/drive/MyDrive/Fatigue_Set'\n",
        "\n",
        "# Create destination folder if it doesn't exist\n",
        "os.makedirs(drive_dest, exist_ok=True)\n",
        "\n",
        "# Define full destination path for the folder copy\n",
        "dest_dir = os.path.join(drive_dest, 'fatigueset')\n",
        "\n",
        "# Remove destination folder if it exists to avoid copytree error\n",
        "if os.path.exists(dest_dir):\n",
        "    shutil.rmtree(dest_dir)\n",
        "\n",
        "# Recursively copy entire directory\n",
        "shutil.copytree(source_dir, dest_dir)\n",
        "\n",
        "print(f'Dataset folder copied recursively to: {dest_dir}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ashWAbDg29UZ"
      },
      "source": [
        "Focused files and column structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buRNFJZd-1nf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Fatigue_Set/fatigueset'\n",
        "persons = [f'{i:02d}' for i in range(1, 13)]\n",
        "sessions = [f'{i:02d}' for i in range(1, 4)]\n",
        "\n",
        "# Selected sensor files relevant for your fatigue detection model\n",
        "selected_sensor_files = [\n",
        "    'wrist_hr.csv',\n",
        "    'wrist_ibi.csv',\n",
        "    'wrist_acc.csv',\n",
        "    'wrist_eda.csv',\n",
        "    'wrist_skin_temperature.csv',\n",
        "    'exp_fatigue.csv'\n",
        "]\n",
        "\n",
        "for person in persons:\n",
        "    for session in sessions:\n",
        "        session_folder = os.path.join(BASE_PATH, person, session)\n",
        "        print(f'\\nPerson: {person}, Session: {session}')\n",
        "        for sensor_file in selected_sensor_files:\n",
        "            file_path = os.path.join(session_folder, sensor_file)\n",
        "            if os.path.exists(file_path):\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, nrows=3)  # Read only first few rows\n",
        "                    print(f'{sensor_file} columns: {list(df.columns)}')\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f'{sensor_file} not found in session {session} of person {person}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3LB-HcG3Del"
      },
      "source": [
        "Focused columns sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fqgUYpuK4dw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Fatigue_Set/fatigueset'\n",
        "PERSONS = [f'{i:02d}' for i in range(1, 13)]\n",
        "SESSIONS = [f'{i:02d}' for i in range(1, 4)]\n",
        "\n",
        "SENSOR_FILES = {\n",
        "    'wrist_hr.csv': ['hr'],\n",
        "    'wrist_ibi.csv': ['duration'],\n",
        "    'wrist_acc.csv': ['ax', 'ay', 'az'],\n",
        "    'wrist_eda.csv': ['eda'],\n",
        "    'wrist_skin_temperature.csv': ['temp']\n",
        "}\n",
        "\n",
        "WINDOW_SIZE_SEC = 30\n",
        "STEP_SIZE_SEC = 15\n",
        "\n",
        "def get_min_sampling_interval(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    if 'timestamp' not in df.columns or df.empty:\n",
        "        return None\n",
        "    ts = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    intervals = ts.diff().dropna()\n",
        "    min_interval = intervals.min()\n",
        "    return min_interval\n",
        "\n",
        "def load_and_resample(filepath, cols, resample_freq):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    original_counts = df[cols].count()\n",
        "\n",
        "    df_resampled = df[cols].resample(resample_freq).mean().interpolate()\n",
        "    resampled_counts = df_resampled.count()\n",
        "\n",
        "    print(f\"File: {os.path.basename(filepath)}\")\n",
        "    for col in cols:\n",
        "        orig = original_counts[col]\n",
        "        resampled = resampled_counts[col]\n",
        "        percent = (resampled / orig * 100) if orig > 0 else 0\n",
        "        print(f\"  Column: {col}, Original entries: {orig}, Resampled entries: {resampled}, Percentage: {percent:.2f}%\")\n",
        "    return df_resampled\n",
        "\n",
        "def load_and_merge_session(person, session):\n",
        "    session_path = os.path.join(BASE_PATH, person, session)\n",
        "    sensor_min_intervals = []\n",
        "\n",
        "    for file_name in SENSOR_FILES.keys():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            min_intv = get_min_sampling_interval(file_path)\n",
        "            if min_intv is not None:\n",
        "                sensor_min_intervals.append(min_intv)\n",
        "    if not sensor_min_intervals:\n",
        "        print(f\"No sensor data found for person {person} session {session}\")\n",
        "        return None\n",
        "\n",
        "    best_interval = max(sensor_min_intervals)\n",
        "    resample_milliseconds = int(best_interval.total_seconds() * 1000)\n",
        "    resample_freq_str = f\"{resample_milliseconds}ms\"\n",
        "    print(f\"\\nPerson {person}, Session {session}, Resampling freq chosen: every {resample_freq_str}\")\n",
        "\n",
        "    data_frames = []\n",
        "    for file_name, cols in SENSOR_FILES.items():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            df_resampled = load_and_resample(file_path, cols, resample_freq_str)\n",
        "            data_frames.append(df_resampled)\n",
        "    if not data_frames:\n",
        "        return None\n",
        "\n",
        "    merged_df = pd.concat(data_frames, axis=1).interpolate().dropna()\n",
        "    print(f\"\\nSynchronized merged data sample for Person {person} Session {session}:\")\n",
        "    print(merged_df.head(20))\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "# Run for all persons and sessions\n",
        "for person in PERSONS:\n",
        "    for session in SESSIONS:\n",
        "        print(f\"Processing Person {person}, Session {session}\")\n",
        "        merged_data = load_and_merge_session(person, session)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7iwjqTJ3Lcz"
      },
      "source": [
        "Features , Labels , Count data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cTbWWd8jpqQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Fatigue_Set/fatigueset'\n",
        "PERSONS   = [f'{i:02d}' for i in range(1, 13)]\n",
        "SESSIONS  = [f'{i:02d}' for i in range(1, 4)]\n",
        "\n",
        "SENSOR_FILES = {\n",
        "    'wrist_hr.csv': ['hr'],\n",
        "    'wrist_ibi.csv': ['duration'],\n",
        "    'wrist_acc.csv': ['ax', 'ay', 'az'],\n",
        "    'wrist_eda.csv': ['eda'],\n",
        "    'wrist_skin_temperature.csv': ['temp']\n",
        "}\n",
        "\n",
        "WINDOW_SIZE_SEC = 30\n",
        "STEP_SIZE_SEC   = 15\n",
        "\n",
        "session_type_map = {\n",
        "    '01': 0,  # baseline\n",
        "    '02': 1,  # physical\n",
        "    '03': 2   # mental\n",
        "}\n",
        "\n",
        "# ========== Utilities for Data Processing ==========\n",
        "\n",
        "def get_min_sampling_interval(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    if 'timestamp' not in df.columns or df.empty:\n",
        "        return None\n",
        "    ts = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    intervals = ts.diff().dropna()\n",
        "    return intervals.min()\n",
        "\n",
        "def load_and_resample(filepath, cols, resample_freq):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df = df.dropna(subset=['timestamp'])\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    df.set_index('timestamp', inplace=True)\n",
        "\n",
        "    df_resampled = df[cols].resample(resample_freq).mean().interpolate()\n",
        "    return df_resampled\n",
        "\n",
        "def load_and_merge_session(person, session):\n",
        "    session_path = os.path.join(BASE_PATH, person, session)\n",
        "    sensor_min_intervals = []\n",
        "    for file_name in SENSOR_FILES.keys():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            min_intv = get_min_sampling_interval(file_path)\n",
        "            if min_intv is not None:\n",
        "                sensor_min_intervals.append(min_intv)\n",
        "    if not sensor_min_intervals:\n",
        "        return None, None, None\n",
        "\n",
        "    best_interval = max(sensor_min_intervals)\n",
        "    resample_milliseconds = int(best_interval.total_seconds() * 1000)\n",
        "    resample_freq_str = f\"{resample_milliseconds}ms\"\n",
        "\n",
        "    data_frames = []\n",
        "    for file_name, cols in SENSOR_FILES.items():\n",
        "        file_path = os.path.join(session_path, file_name)\n",
        "        if os.path.exists(file_path):\n",
        "            df_resampled = load_and_resample(file_path, cols, resample_freq_str)\n",
        "            data_frames.append(df_resampled)\n",
        "    if not data_frames:\n",
        "        return None, None, None\n",
        "\n",
        "    merged_df = pd.concat(data_frames, axis=1).interpolate().dropna()\n",
        "    return merged_df, None, None\n",
        "\n",
        "def windowed_segmentation(data, window_size_sec=30, step_size_sec=15, fs_hz=None):\n",
        "    if fs_hz is None:\n",
        "        timedelta = (data.index[1] - data.index[0]).total_seconds()\n",
        "        fs_hz = 1 / timedelta\n",
        "\n",
        "    window_size_samples = int(window_size_sec * fs_hz)\n",
        "    step_size_samples   = int(step_size_sec * fs_hz)\n",
        "\n",
        "    segments, indices = [], []\n",
        "    for start in range(0, len(data) - window_size_samples + 1, step_size_samples):\n",
        "        end = start + window_size_samples\n",
        "        segment = data.iloc[start:end]\n",
        "        segments.append(segment)\n",
        "        indices.append(segment.index[0])\n",
        "    return segments, indices\n",
        "\n",
        "def extract_features(segment):\n",
        "    features = {}\n",
        "    for col in segment.columns:\n",
        "        features[f'{col}_mean'] = segment[col].mean()\n",
        "        features[f'{col}_std']  = segment[col].std()\n",
        "    return features\n",
        "\n",
        "def load_fatigue_labels(person, session, session_start_timestamp):\n",
        "    fatigue_path = os.path.join(BASE_PATH, person, session, 'exp_fatigue.csv')\n",
        "    if not os.path.exists(fatigue_path):\n",
        "        return None\n",
        "    df = pd.read_csv(fatigue_path)\n",
        "    df['fatigueSurveySubmissionDatetime'] = df['fatigueSurveySubmissionTime'].apply(\n",
        "        lambda x: pd.Timestamp(session_start_timestamp) + pd.Timedelta(seconds=x)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "def align_labels_to_windows_time_based(label_df, window_starts):\n",
        "    \"\"\"\n",
        "    Instead of nearest label assignment, interpolate fatigue scores over time.\n",
        "    - Assumes label_df has 'fatigueSurveySubmissionDatetime',\n",
        "      'physicalFatigueScore', 'mentalFatigueScore'\n",
        "    - window_starts is a list/array of pandas Timestamps\n",
        "    \"\"\"\n",
        "    submission_times = pd.to_datetime(label_df['fatigueSurveySubmissionDatetime'])\n",
        "    phys_scores = label_df['physicalFatigueScore'].values\n",
        "    ment_scores = label_df['mentalFatigueScore'].values\n",
        "\n",
        "    # Build interpolation functions (time → fatigue score)\n",
        "    phys_interp = np.interp(\n",
        "        [ts.value for ts in window_starts],     # convert to ns int\n",
        "        [t.value for t in submission_times],\n",
        "        phys_scores\n",
        "    )\n",
        "    ment_interp = np.interp(\n",
        "        [ts.value for ts in window_starts],\n",
        "        [t.value for t in submission_times],\n",
        "        ment_scores\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for p, m in zip(phys_interp, ment_interp):\n",
        "        labels.append({\n",
        "            'physicalFatigueScore': p,\n",
        "            'mentalFatigueScore': m\n",
        "        })\n",
        "    return labels\n",
        "\n",
        "\n",
        "def process_person_session(person, session):\n",
        "    merged_df, _, _ = load_and_merge_session(person, session)\n",
        "    if merged_df is None:\n",
        "        return None\n",
        "\n",
        "    ts_deltas = merged_df.index.to_series().diff().dropna()\n",
        "    fs_hz = 1 / ts_deltas.mean().total_seconds()\n",
        "\n",
        "    windows, window_starts = windowed_segmentation(merged_df, WINDOW_SIZE_SEC, STEP_SIZE_SEC, fs_hz)\n",
        "\n",
        "    feature_list = [extract_features(window) for window in windows]\n",
        "    features_df = pd.DataFrame(feature_list)\n",
        "\n",
        "    session_start_timestamp = merged_df.index.min()\n",
        "    fatigue_labels_df = load_fatigue_labels(person, session, session_start_timestamp)\n",
        "    if fatigue_labels_df is None:\n",
        "        return None\n",
        "\n",
        "    labels = align_labels_to_windows_time_based(fatigue_labels_df, window_starts)\n",
        "    labels_df = pd.DataFrame(labels)\n",
        "\n",
        "    merged_features_labels_df = pd.concat([features_df.reset_index(drop=True),\n",
        "                                           labels_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    merged_features_labels_df['window_start'] = pd.to_datetime(window_starts).values\n",
        "    merged_features_labels_df['person'] = person\n",
        "    merged_features_labels_df['session'] = session\n",
        "    return merged_features_labels_df\n",
        "\n",
        "# ========== Build Whole Dataset ==========\n",
        "\n",
        "all_data = []\n",
        "for person in PERSONS:\n",
        "    for session in SESSIONS:\n",
        "        df = process_person_session(person, session)\n",
        "        if df is not None:\n",
        "            all_data.append(df)\n",
        "\n",
        "final_df = pd.concat(all_data, ignore_index=True)\n",
        "print(\"Final dataframe shape:\", final_df.shape)\n",
        "\n",
        "save_path = '/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_interpolated.csv'\n",
        "final_df.to_csv(save_path, index=False)\n",
        "print(f\"Saved merged dataset to {save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GpMBE2m2m6d"
      },
      "outputs": [],
      "source": [
        "ffld=pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_interpolated.csv')\n",
        "ffld.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMepKJDu_yje"
      },
      "source": [
        "Data Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKHsgFUW5JgS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import random\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_interpolated.csv')\n",
        "\n",
        "# Sensor feature columns\n",
        "feature_cols = [\n",
        "    'hr_mean', 'hr_std', 'duration_mean', 'duration_std',\n",
        "    'ax_mean', 'ax_std', 'ay_mean', 'ay_std', 'az_mean', 'az_std',\n",
        "    'eda_mean', 'eda_std', 'temp_mean', 'temp_std'\n",
        "]\n",
        "\n",
        "# Target label columns\n",
        "label_cols = ['physicalFatigueScore', 'mentalFatigueScore']\n",
        "\n",
        "# ---- Hold-out split BEFORE scaling ----\n",
        "all_pairs = df.groupby(['person','session']).size().index.tolist()\n",
        "random.seed(42); random.shuffle(all_pairs)\n",
        "train_pairs, test_pairs = all_pairs[:31], all_pairs[31:]\n",
        "\n",
        "train_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p,s) in train_pairs])\n",
        "test_df  = pd.concat([df[(df.person==p) & (df.session==s)] for (p,s) in test_pairs])\n",
        "\n",
        "# ---- Fit scaler only on training data ----\n",
        "feat_scaler = MinMaxScaler()\n",
        "train_df[feature_cols] = feat_scaler.fit_transform(train_df[feature_cols])\n",
        "test_df[feature_cols]  = feat_scaler.transform(test_df[feature_cols])\n",
        "joblib.dump(feat_scaler, 'feature_scaler.save')\n",
        "\n",
        "# ---- Scale labels (optional: usually for regression stability) ----\n",
        "label_scaler = MinMaxScaler()\n",
        "train_df[label_cols] = label_scaler.fit_transform(train_df[label_cols])\n",
        "test_df[label_cols]  = label_scaler.transform(test_df[label_cols])\n",
        "joblib.dump(label_scaler, 'label_scaler.save')\n",
        "\n",
        "# ---- Save normalized train/test sets ----\n",
        "train_df.to_csv('/content/drive/MyDrive/Fatigue_Set/final_train_normalized.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/Fatigue_Set/final_test_normalized.csv', index=False)\n",
        "\n",
        "print(\"Features + Labels normalized (train/test separately) and saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFr5fxlo329i"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_train_normalized.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI-9aamGAzgx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "\n",
        "# Count entries per (person, session)\n",
        "counts = df.groupby(['person', 'session']).size().reset_index(name='count')\n",
        "\n",
        "print(counts)\n",
        "\n",
        "total_counts = df.groupby('person').size().reset_index(name='total_count')\n",
        "\n",
        "print(total_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfqPxVtJ_1lG"
      },
      "source": [
        "#Single Client - train with 3 X 3 sessions , test with 1 X 1 session (Desired model is after time sequencing model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFb81XEhyym9"
      },
      "source": [
        "Base line model (without time sequencing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnhQ1sHl7Kx-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "\n",
        "# ========== Model Components ==========\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, n_modalities, hidden_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        concat_dim = n_modalities * hidden_dim\n",
        "        self.query = nn.Linear(concat_dim, fusion_dim)\n",
        "        self.key   = nn.Linear(concat_dim, fusion_dim)\n",
        "        self.value = nn.Linear(concat_dim, fusion_dim)\n",
        "    def forward(self, features, domain_disc):\n",
        "        x = torch.cat(features, dim=1)\n",
        "        Q, K, V = self.query(x), self.key(x), self.value(x)\n",
        "        attn_scores = torch.matmul(Q, K.T) / (K.size(-1) ** 0.5)\n",
        "        attn_scores = attn_scores + domain_disc\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        out = torch.matmul(attn_weights, V)\n",
        "        return out\n",
        "\n",
        "class GradReverse(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambd):\n",
        "        ctx.lambd = lambd\n",
        "        return x.view_as(x)\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.lambd, None\n",
        "\n",
        "class GradientReversal(nn.Module):\n",
        "    def __init__(self, lambd=1.0):\n",
        "        super().__init__()\n",
        "        self.lambd = lambd\n",
        "    def forward(self, x):\n",
        "        return GradReverse.apply(x, self.lambd)\n",
        "\n",
        "class DomainAdaptiveLayer(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.grl = GradientReversal()\n",
        "        self.fc = nn.Linear(input_dim, input_dim)\n",
        "    def forward(self, x):\n",
        "        return self.fc(self.grl(x))\n",
        "\n",
        "class FMAL_Daf(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(inp_dim, lstm_hidden) for inp_dim in modalities_dim])\n",
        "        self.attn_fusion = CrossModalAttention(len(modalities_dim), lstm_hidden, fusion_dim)\n",
        "        self.domain_adapt = DomainAdaptiveLayer(fusion_dim)\n",
        "        self.global_lstm = nn.LSTM(fusion_dim, 32, batch_first=True)\n",
        "        self.reg_head_phys = nn.Linear(32, 1)\n",
        "        self.reg_head_ment = nn.Linear(32, 1)\n",
        "        self.class_head = nn.Linear(32, 3)\n",
        "    def forward(self, modal_inputs, domain_disc):\n",
        "        feats = [mod(mod_inp) for mod, mod_inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = self.attn_fusion(feats, domain_disc)\n",
        "        adapted = self.domain_adapt(fused)\n",
        "        glstm_out, _ = self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_head_phys(feat), self.reg_head_ment(feat), self.class_head(feat)\n",
        "\n",
        "# ========== Dataset Loader ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df.reset_index(drop=True)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'], row['ax_std'], row['ay_mean'], row['ay_std'], row['az_mean'], row['az_std']], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "        phys = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "        session_type = torch.tensor(0, dtype=torch.long)  # placeholder\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)  # placeholder\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, (phys, ment, session_type)\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([sample[0][i] for sample in batch]) for i in range(5)]\n",
        "    domain_disc = torch.stack([sample[1] for sample in batch])\n",
        "    phys = torch.stack([sample[2][0] for sample in batch])\n",
        "    ment = torch.stack([sample[2][1] for sample in batch])\n",
        "    stype = torch.stack([sample[2][2] for sample in batch])\n",
        "    return modal_inputs, domain_disc, (phys, ment, stype)\n",
        "\n",
        "# ========== Data Preparation ==========\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "\n",
        "all_pairs = df.groupby(['person', 'session']).size().index.tolist()\n",
        "\n",
        "# One client: 3 persons × 3 sessions = 9 pairs\n",
        "train_pairs = all_pairs[:9]\n",
        "# One test pair\n",
        "test_pair = all_pairs[9]\n",
        "\n",
        "# ✅ Print which person–session pairs are chosen\n",
        "print(\"Training person-session pairs:\")\n",
        "for p, s in train_pairs:\n",
        "    print(f\"  Person {p}, Session {s}\")\n",
        "\n",
        "print(\"\\nTesting person-session pair:\")\n",
        "print(f\"  Person {test_pair[0]}, Session {test_pair[1]}\")\n",
        "\n",
        "# Build client dataset (only 1 client here)\n",
        "client_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p, s) in train_pairs]).drop(columns=['person'])\n",
        "clients_datasets = [FatigueSessionDataset(client_df)]\n",
        "\n",
        "# Test dataset\n",
        "test_df = df[(df.person == test_pair[0]) & (df.session == test_pair[1])]\n",
        "test_df= test_df.drop(columns=['person'])\n",
        "test_dataset = FatigueSessionDataset(test_df)\n",
        "\n",
        "# Loaders\n",
        "clients_loaders = [DataLoader(clients_datasets[0], batch_size=32, shuffle=True, collate_fn=collate)]\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# ========== Training ==========\n",
        "modal_dims = [2, 2, 6, 2, 2]\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "global_model = FMAL_Daf(modal_dims).to(device)\n",
        "\n",
        "criterion_reg = nn.MSELoss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_one_client(model, loader, client_id, rnd):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    total_loss = 0\n",
        "    for batch_idx, (modal_inputs, domain_disc, (phys, ment, stype)) in enumerate(loader):\n",
        "        modal_inputs = [x.to(device) for x in modal_inputs]\n",
        "        domain_disc = domain_disc.to(device)\n",
        "        phys, ment, stype = phys.to(device), ment.to(device), stype.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        phys_pred, ment_pred, stype_pred = model(modal_inputs, domain_disc)\n",
        "        loss = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "        loss += criterion_cls(stype_pred, stype)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"[Round {rnd+1}] Client {client_id+1} Avg Training Loss: {avg_loss:.4f}\")\n",
        "    return model.state_dict()\n",
        "\n",
        "optimizer = torch.optim.Adam(global_model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(10):\n",
        "    global_model.train()\n",
        "    total_loss = 0\n",
        "    for modal_inputs, domain_disc, (phys, ment, stype) in clients_loaders[0]:\n",
        "        modal_inputs = [x.to(device) for x in modal_inputs]\n",
        "        domain_disc = domain_disc.to(device)\n",
        "        phys, ment, stype = phys.to(device), ment.to(device), stype.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        phys_pred, ment_pred, stype_pred = global_model(modal_inputs, domain_disc)\n",
        "        loss = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "        loss += criterion_cls(stype_pred, stype)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Avg Training Loss: {total_loss / len(clients_loaders[0]):.4f}\")\n",
        "\n",
        "\n",
        "# ========== Testing ==========\n",
        "global_model.eval()\n",
        "reg_losses, cls_losses = [], []\n",
        "with torch.no_grad():\n",
        "    for modal_inputs, domain_disc, (phys, ment, stype) in test_loader:\n",
        "        modal_inputs = [x.to(device) for x in modal_inputs]\n",
        "        domain_disc = domain_disc.to(device)\n",
        "        phys, ment, stype = phys.to(device), ment.to(device), stype.to(device)\n",
        "        phys_pred, ment_pred, stype_pred = global_model(modal_inputs, domain_disc)\n",
        "        reg_loss = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "        cls_loss = criterion_cls(stype_pred, stype)\n",
        "        reg_losses.append(reg_loss.item())\n",
        "        cls_losses.append(cls_loss.item())\n",
        "\n",
        "print(\"\\n=== Final Test Performance ===\")\n",
        "print(f\"Test Regression Loss: {np.mean(reg_losses):.4f}\")\n",
        "print(f\"Test Classification Loss: {np.mean(cls_losses):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtClHndtyivz"
      },
      "source": [
        "PREVIOUSLY WITHOUT TIME SEQUENCING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjIQCqR7ZdH3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# ========== Dataset Loader ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.data = df.reset_index(drop=True)\n",
        "        self.session_map = {1: 0, 2: 1, 3: 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # --- Features ---\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'], row['ax_std'],\n",
        "                            row['ay_mean'], row['ay_std'],\n",
        "                            row['az_mean'], row['az_std']], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        # --- Labels ---\n",
        "        phys = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "\n",
        "        session_type = torch.tensor(self.session_map[row['session']], dtype=torch.long)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)  # placeholder\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, (phys, ment, session_type)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([sample[0][i] for sample in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([sample[1] for sample in batch])\n",
        "    phys         = torch.stack([sample[2][0] for sample in batch])\n",
        "    ment         = torch.stack([sample[2][1] for sample in batch])\n",
        "    stype        = torch.stack([sample[2][2] for sample in batch])\n",
        "    return modal_inputs, domain_disc, (phys, ment, stype)\n",
        "\n",
        "\n",
        "# ========== Model Architecture ==========\n",
        "\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, n_modalities, hidden_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        # Instead of concatenation, we expect [batch, n_modalities, hidden_dim]\n",
        "        self.query = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.key   = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.value = nn.Linear(hidden_dim, fusion_dim)\n",
        "\n",
        "    def forward(self, features, domain_disc):\n",
        "        # features: list of modality outputs, each [batch, hidden_dim]\n",
        "        # Stack to [batch, n_modalities, hidden_dim]\n",
        "        x = torch.stack(features, dim=1)\n",
        "\n",
        "        Q = self.query(x)  # [batch, n_modalities, fusion_dim]\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Compute attention scores along modality dim\n",
        "        # attn_scores shape: [batch, n_modalities, n_modalities]\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "\n",
        "        # domain_disc shape is [batch, 1] or [batch, dim], broadcasting accordingly\n",
        "        # Add domain distribution bias if provided\n",
        "        if domain_disc is not None and domain_disc.numel() > 1:\n",
        "            # reshape if needed\n",
        "            domain_disc = domain_disc.unsqueeze(1).expand(-1, attn_scores.size(1), attn_scores.size(2))\n",
        "            attn_scores = attn_scores + domain_disc\n",
        "        else:\n",
        "            # domain_disc is placeholder scalar, ignore\n",
        "            pass\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)  # weights over modalities\n",
        "        out = torch.matmul(attn_weights, V)  # [batch, n_modalities, fusion_dim]\n",
        "\n",
        "        # Aggregate modalities by averaging weighted outputs per sample\n",
        "        out = out.mean(dim=1)  # [batch, fusion_dim]\n",
        "        return out\n",
        "\n",
        "\n",
        "class FMAL_Daf_Modified(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=32, fusion_dim=64, use_grl=False):\n",
        "        super().__init__()\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(inp_dim, lstm_hidden) for inp_dim in modalities_dim])\n",
        "        self.attn_fusion = CrossModalAttention(len(modalities_dim), lstm_hidden, fusion_dim)\n",
        "        self.attn_dropout = nn.Dropout(0.2)  # Added dropout\n",
        "        # Domain adaptation disabled for debugging\n",
        "        self.domain_adapt = nn.Identity() if not use_grl else nn.Linear(fusion_dim, fusion_dim)\n",
        "        self.global_lstm = nn.LSTM(fusion_dim, 32, batch_first=True)\n",
        "        self.reg_head_phys = nn.Linear(32, 1)\n",
        "        self.reg_head_ment = nn.Linear(32, 1)\n",
        "        self.class_head = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, modal_inputs, domain_disc):\n",
        "        feats = [mod(mod_inp) for mod, mod_inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = self.attn_fusion(feats, domain_disc)\n",
        "        fused = self.attn_dropout(fused)\n",
        "        adapted = self.domain_adapt(fused)\n",
        "        glstm_out, _ = self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_head_phys(feat), self.reg_head_ment(feat), self.class_head(feat)\n",
        "\n",
        "\n",
        "# ========== Training & Evaluation Functions ==========\n",
        "def train_model(model, loader, optimizer, criterion_reg, criterion_cls, epochs=10):\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for modal_inputs, domain_disc, (phys, ment, stype) in loader:\n",
        "            modal_inputs = [x for x in modal_inputs]\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move domain_disc to device if not None\n",
        "            domain_disc = domain_disc.to(modal_inputs[0].device)\n",
        "\n",
        "            phys_pred, ment_pred, cls_pred = model(modal_inputs, domain_disc)\n",
        "\n",
        "            loss_reg = criterion_reg(phys_pred, phys) + criterion_reg(ment_pred, ment)\n",
        "            loss_cls = criterion_cls(cls_pred, stype)\n",
        "            loss = loss_reg + 0.1 * loss_cls  # Scale classification loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader)\n",
        "        print(f\"[Epoch {epoch}] Avg Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model, test_loader, device, criterion_reg, criterion_cls):\n",
        "    model.eval()\n",
        "    total_reg_loss, total_cls_loss = 0.0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            features, domain_disc, (true_phys, true_ment, true_stype) = batch\n",
        "            features = [x.to(device) for x in features]\n",
        "            true_phys = true_phys.to(device)\n",
        "            true_ment = true_ment.to(device)\n",
        "            true_stype = true_stype.to(device)\n",
        "\n",
        "            domain_disc = domain_disc.to(device)\n",
        "\n",
        "            pred_phys, pred_ment, pred_cls = model(features, domain_disc)\n",
        "            loss_reg = criterion_reg(pred_phys, true_phys) + criterion_reg(pred_ment, true_ment)\n",
        "            loss_cls = criterion_cls(pred_cls, true_stype)\n",
        "\n",
        "            total_reg_loss += loss_reg.item()\n",
        "            total_cls_loss += loss_cls.item()\n",
        "\n",
        "            pred_cls_labels = torch.argmax(pred_cls, dim=1)\n",
        "\n",
        "            print(\"=== Predictions vs Ground Truth ===\")\n",
        "            for i in range(len(pred_phys)):\n",
        "                print(f\"Sample {i}:\")\n",
        "                print(f\"  True phys={true_phys[i].item():.3f}, True ment={true_ment[i].item():.3f}, \"\n",
        "                      f\"True session_type={true_stype[i].item()}\")\n",
        "                print(f\"  Pred phys={pred_phys[i].item():.3f}, Pred ment={pred_ment[i].item():.3f}, \"\n",
        "                      f\"Pred session_type={pred_cls_labels[i].item()}\")\n",
        "            break  # only first batch\n",
        "\n",
        "    avg_reg_loss = total_reg_loss / len(test_loader)\n",
        "    avg_cls_loss = total_cls_loss / len(test_loader)\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss: {avg_reg_loss:.4f}\")\n",
        "    print(f\"Test Classification Loss: {avg_cls_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ========== Main ==========\n",
        "df = pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "\n",
        "# Extract all unique (person, session) pairs\n",
        "all_pairs = df.groupby(['person', 'session']).size().index.tolist()\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(all_pairs)\n",
        "\n",
        "# Select 9 pairs for training and 3 for testing\n",
        "train_pairs = all_pairs[:9]\n",
        "test_pairs = all_pairs[9:12]  # next 3 pairs\n",
        "\n",
        "print(\"Training person-session pairs:\")\n",
        "for (p, s) in train_pairs:\n",
        "    print(f\"  Person {int(p)}, Session {int(s)}\")\n",
        "\n",
        "print(\"\\nTesting person-session pairs:\")\n",
        "for (p, s) in test_pairs:\n",
        "    print(f\"  Person {int(p)}, Session {int(s)}\")\n",
        "\n",
        "# Concatenate data for train and test sets respectively\n",
        "train_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p, s) in train_pairs]).drop(columns=['person'])\n",
        "test_df = pd.concat([df[(df.person==p) & (df.session==s)] for (p, s) in test_pairs]).drop(columns=['person'])\n",
        "\n",
        "train_dataset = FatigueSessionDataset(train_df)\n",
        "test_dataset = FatigueSessionDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "print(\"\\n=== Sample from Training Data ===\")\n",
        "for i in range(3):\n",
        "    features, _, (phys, ment, stype) = train_dataset[i]\n",
        "    print(f\"Sample {i}: phys={phys.item():.3f}, ment={ment.item():.3f}, session_type={stype.item()}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "modal_dims = [2, 2, 6, 2, 2]\n",
        "model = FMAL_Daf_Modified(modal_dims, use_grl=False).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion_reg = nn.MSELoss()\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model, train_loader, optimizer, criterion_reg, criterion_cls, epochs=10)\n",
        "evaluate_and_show_predictions(model, test_loader, device, criterion_reg, criterion_cls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeZiCeLXynU5"
      },
      "source": [
        "AFTER TIME SEQUENCING (SESSIONS-WISE SEQUENCE-WISE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO7t2hkfx6v7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd, numpy as np, random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "# ========== Dataset ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Groups rows into sessions. Each __getitem__ = one timestep from one session.\n",
        "    Why? -> To keep sample-level printing like your original \"Sample 0: phys=..\"\n",
        "    \"\"\"\n",
        "    def __init__(self, df, session_map=None):\n",
        "        self.data = df.sort_values([\"session\",\"window_start\"]).reset_index(drop=True)\n",
        "        self.session_map = session_map if session_map else {1:0,2:1,3:2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'],row['ax_std'],row['ay_mean'],row['ay_std'],row['az_mean'],row['az_std']],dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        phys = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "        stype = torch.tensor(self.session_map[row['session']], dtype=torch.long)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)\n",
        "        return [HR,IBI,ACC,EDA,Temp], domain_disc, (phys,ment,stype)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([b[0][i] for b in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([b[1] for b in batch])\n",
        "    phys         = torch.stack([b[2][0] for b in batch])\n",
        "    ment         = torch.stack([b[2][1] for b in batch])\n",
        "    stype        = torch.stack([b[2][2] for b in batch])\n",
        "    return modal_inputs, domain_disc, (phys,ment,stype)\n",
        "\n",
        "\n",
        "# ========== Model ==========\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self,input_dim,hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim,hidden_dim,batch_first=True)\n",
        "    def forward(self,x):\n",
        "        if x.ndim==2:\n",
        "            x = x.unsqueeze(1)      # single-step fallback\n",
        "        out,_ = self.lstm(x)\n",
        "        return out[:,-1,:]\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self,n_modalities,hidden_dim=32,fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim,fusion_dim)\n",
        "        self.key   = nn.Linear(hidden_dim,fusion_dim)\n",
        "        self.value = nn.Linear(hidden_dim,fusion_dim)\n",
        "    def forward(self,features,domain_disc):\n",
        "        x = torch.stack(features,1)          # [B,M,H]\n",
        "        Q,K,V = self.query(x),self.key(x),self.value(x)\n",
        "        scores = torch.matmul(Q,K.transpose(-2,-1))/(K.size(-1)**0.5)\n",
        "        attn = F.softmax(scores,dim=-1)\n",
        "        out = torch.matmul(attn,V).mean(1)   # [B,Fusion]\n",
        "        return out\n",
        "\n",
        "\n",
        "class DomainAdaptiveLayer(nn.Module):\n",
        "    def __init__(self,dim): super().__init__(); self.fc=nn.Linear(dim,dim)\n",
        "    def forward(self,x): return self.fc(x)\n",
        "\n",
        "\n",
        "class FMAL_Daf(nn.Module):\n",
        "    def __init__(self,modalities_dim,lstm_hidden=32,fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.modality_lstms=nn.ModuleList([ModalityLSTM(d,lstm_hidden) for d in modalities_dim])\n",
        "        self.attn_fusion=CrossModalAttention(len(modalities_dim),lstm_hidden,fusion_dim)\n",
        "        self.domain_adapt=DomainAdaptiveLayer(fusion_dim)\n",
        "        self.global_lstm=nn.LSTM(fusion_dim,32,batch_first=True)\n",
        "        self.reg_phys=nn.Linear(32,1)\n",
        "        self.reg_ment=nn.Linear(32,1)\n",
        "        self.class_head=nn.Linear(32,3)\n",
        "    def forward(self,modal_inputs,domain_disc):\n",
        "        feats=[m(inp) for m,inp in zip(self.modality_lstms,modal_inputs)]\n",
        "        fused=self.attn_fusion(feats,domain_disc)\n",
        "        adapted=self.domain_adapt(fused)\n",
        "        glstm_out,_=self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat=glstm_out[:,-1,:]\n",
        "        return self.reg_phys(feat),self.reg_ment(feat),self.class_head(feat)\n",
        "\n",
        "\n",
        "# ========== Train / Eval ==========\n",
        "def train_model(model,loader,optim,crit_reg,crit_cls,epochs=10):\n",
        "    for ep in range(1,epochs+1):\n",
        "        model.train(); tot=0\n",
        "        for modals,dom,(phys,ment,stype) in loader:\n",
        "            modals=[m.to(device) for m in modals]\n",
        "            dom=dom.to(device); phys,ment,stype=phys.to(device),ment.to(device),stype.to(device)\n",
        "            optim.zero_grad()\n",
        "            p,m,s=model(modals,dom)\n",
        "            loss=crit_reg(p,phys)+crit_reg(m,ment)+0.1*crit_cls(s,stype)\n",
        "            loss.backward(); optim.step(); tot+=loss.item()\n",
        "        print(f\"[Epoch {ep}] Avg Training Loss: {tot/len(loader):.4f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model,test_loader,device,crit_reg,crit_cls):\n",
        "    model.eval(); regL,clsL=0,0\n",
        "    with torch.no_grad():\n",
        "        for modals,dom,(phys,ment,stype) in test_loader:\n",
        "            modals=[m.to(device) for m in modals]\n",
        "            phys,ment,stype=phys.to(device),ment.to(device),stype.to(device)\n",
        "            dom=dom.to(device)\n",
        "            p_pred,m_pred,s_pred=model(modals,dom)\n",
        "            loss_r=crit_reg(p_pred,phys)+crit_reg(m_pred,ment)\n",
        "            loss_c=crit_cls(s_pred,stype)\n",
        "            regL+=loss_r.item(); clsL+=loss_c.item()\n",
        "            pred_cls=torch.argmax(s_pred,1)\n",
        "            print(\"=== Predictions vs Ground Truth ===\")\n",
        "            for i in range(len(phys)):\n",
        "                print(f\"Sample {i}:\")\n",
        "                print(f\"  True phys={phys[i].item():.3f}, True ment={ment[i].item():.3f}, True session_type={stype[i].item()}\")\n",
        "                print(f\"  Pred phys={p_pred[i].item():.3f}, Pred ment={m_pred[i].item():.3f}, Pred session_type={pred_cls[i].item()}\")\n",
        "            break\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss: {regL/len(test_loader):.4f}\")\n",
        "    print(f\"Test Classification Loss: {clsL/len(test_loader):.4f}\")\n",
        "\n",
        "\n",
        "# ========== Main ==========\n",
        "df=pd.read_csv('/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized.csv')\n",
        "all_pairs=df.groupby(['person','session']).size().index.tolist()\n",
        "random.seed(42); random.shuffle(all_pairs)\n",
        "train_pairs,test_pairs=all_pairs[:9],all_pairs[9:12]\n",
        "\n",
        "print(\"Training person-session pairs:\")\n",
        "for p,s in train_pairs: print(f\"  Person {p}, Session {s}\")\n",
        "print(\"\\nTesting person-session pairs:\")\n",
        "for p,s in test_pairs: print(f\"  Person {p}, Session {s}\")\n",
        "\n",
        "train_df=pd.concat([df[(df.person==p)&(df.session==s)] for (p,s) in train_pairs])\n",
        "test_df=pd.concat([df[(df.person==p)&(df.session==s)] for (p,s) in test_pairs])\n",
        "\n",
        "train_dataset=FatigueSessionDataset(train_df)\n",
        "test_dataset=FatigueSessionDataset(test_df)\n",
        "train_loader=DataLoader(train_dataset,batch_size=32,shuffle=True,collate_fn=collate)\n",
        "test_loader=DataLoader(test_dataset,batch_size=32,shuffle=False,collate_fn=collate)\n",
        "\n",
        "print(\"\\n=== Sample from Training Data ===\")\n",
        "for i in range(3):\n",
        "    feats,_,(phys,ment,stype)=train_dataset[i]\n",
        "    print(f\"Sample {i}: phys={phys.item():.3f}, ment={ment.item():.3f}, session_type={stype.item()}\")\n",
        "\n",
        "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=FMAL_Daf([2,2,6,2,2]).to(device)\n",
        "optim=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "crit_reg,crit_cls=nn.MSELoss(),nn.CrossEntropyLoss()\n",
        "\n",
        "train_model(model,train_loader,optim,crit_reg,crit_cls,epochs=10)\n",
        "evaluate_and_show_predictions(model,test_loader,device,crit_reg,crit_cls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7dTyuwYbTub"
      },
      "source": [
        "#Centralized Model - With Novelty\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve5tDceWbQ14",
        "outputId": "b81892cd-0c20-4f2d-de01-bc5c6df75083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded data from /content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized_interpolated.csv. Shape: (2819, 19)\n",
            "Found 3 unique sessions.\n",
            "Training set size: 2255\n",
            "Test set size: 564\n",
            "Using device: cpu\n",
            "[Epoch 1] Avg Training Loss: 0.357502\n",
            "[Epoch 1] Evaluation Metrics:\n",
            "{'loss': 0.0962678889433543,\n",
            " 'mae_mental': 0.17874468863010406,\n",
            " 'mae_physical': 0.17359048128128052,\n",
            " 'r2_mental': 0.0024542808532714844,\n",
            " 'r2_physical': 0.004254043102264404,\n",
            " 'rmse_mental': 0.2262439296761638,\n",
            " 'rmse_physical': 0.21509712190768646}\n",
            "[Epoch 2] Avg Training Loss: 0.148031\n",
            "[Epoch 2] Evaluation Metrics:\n",
            "{'loss': 0.08527866311164366,\n",
            " 'mae_mental': 0.16470257937908173,\n",
            " 'mae_physical': 0.156401127576828,\n",
            " 'r2_mental': 0.10842150449752808,\n",
            " 'r2_physical': 0.1252657175064087,\n",
            " 'rmse_mental': 0.21388992641573712,\n",
            " 'rmse_physical': 0.20160365756431362}\n",
            "[Epoch 3] Avg Training Loss: 0.143667\n",
            "[Epoch 3] Evaluation Metrics:\n",
            "{'loss': 0.08531244357840882,\n",
            " 'mae_mental': 0.16737863421440125,\n",
            " 'mae_physical': 0.15157687664031982,\n",
            " 'r2_mental': 0.0968400239944458,\n",
            " 'r2_physical': 0.1325015425682068,\n",
            " 'rmse_mental': 0.21527463879057138,\n",
            " 'rmse_physical': 0.20076808403872995}\n",
            "[Epoch 4] Avg Training Loss: 0.144161\n",
            "[Epoch 4] Evaluation Metrics:\n",
            "{'loss': 0.0903224246059027,\n",
            " 'mae_mental': 0.16009025275707245,\n",
            " 'mae_physical': 0.1702238917350769,\n",
            " 'r2_mental': 0.11123985052108765,\n",
            " 'r2_physical': 0.015928983688354492,\n",
            " 'rmse_mental': 0.21355159763566076,\n",
            " 'rmse_physical': 0.21383240821961108}\n",
            "[Epoch 5] Avg Training Loss: 0.141177\n",
            "[Epoch 5] Evaluation Metrics:\n",
            "{'loss': 0.0835578650650051,\n",
            " 'mae_mental': 0.1654800921678543,\n",
            " 'mae_physical': 0.1484343707561493,\n",
            " 'r2_mental': 0.10616880655288696,\n",
            " 'r2_physical': 0.16274350881576538,\n",
            " 'rmse_mental': 0.21415996107935328,\n",
            " 'rmse_physical': 0.1972375415413937}\n",
            "[Epoch 6] Avg Training Loss: 0.137112\n",
            "[Epoch 6] Evaluation Metrics:\n",
            "{'loss': 0.08390226163383988,\n",
            " 'mae_mental': 0.15994474291801453,\n",
            " 'mae_physical': 0.14963386952877045,\n",
            " 'r2_mental': 0.12646937370300293,\n",
            " 'r2_physical': 0.13672268390655518,\n",
            " 'rmse_mental': 0.2117140040974236,\n",
            " 'rmse_physical': 0.2002790322516534}\n",
            "[Epoch 7] Avg Training Loss: 0.136518\n",
            "[Epoch 7] Evaluation Metrics:\n",
            "{'loss': 0.08209170173439714,\n",
            " 'mae_mental': 0.15826791524887085,\n",
            " 'mae_physical': 0.16020362079143524,\n",
            " 'r2_mental': 0.16512686014175415,\n",
            " 'r2_physical': 0.13204288482666016,\n",
            " 'rmse_mental': 0.20697637046540404,\n",
            " 'rmse_physical': 0.20082115415060245}\n",
            "[Epoch 8] Avg Training Loss: 0.135240\n",
            "[Epoch 8] Evaluation Metrics:\n",
            "{'loss': 0.08132622960127062,\n",
            " 'mae_mental': 0.16115480661392212,\n",
            " 'mae_physical': 0.15279334783554077,\n",
            " 'r2_mental': 0.14767855405807495,\n",
            " 'r2_physical': 0.16552555561065674,\n",
            " 'rmse_mental': 0.20912802741292885,\n",
            " 'rmse_physical': 0.19690957371325643}\n",
            "[Epoch 9] Avg Training Loss: 0.132840\n",
            "[Epoch 9] Evaluation Metrics:\n",
            "{'loss': 0.08152533984846538,\n",
            " 'mae_mental': 0.16368399560451508,\n",
            " 'mae_physical': 0.1544286012649536,\n",
            " 'r2_mental': 0.14074355363845825,\n",
            " 'r2_physical': 0.16822856664657593,\n",
            " 'rmse_mental': 0.20997710054861388,\n",
            " 'rmse_physical': 0.1965904010444407}\n",
            "[Epoch 10] Avg Training Loss: 0.130026\n",
            "[Epoch 10] Evaluation Metrics:\n",
            "{'loss': 0.07778032424135341,\n",
            " 'mae_mental': 0.15797163546085358,\n",
            " 'mae_physical': 0.14560380578041077,\n",
            " 'r2_mental': 0.17488133907318115,\n",
            " 'r2_physical': 0.21401488780975342,\n",
            " 'rmse_mental': 0.20576368818893054,\n",
            " 'rmse_physical': 0.19110299435055692}\n",
            "[Epoch 11] Avg Training Loss: 0.127380\n",
            "[Epoch 11] Evaluation Metrics:\n",
            "{'loss': 0.07922727035151587,\n",
            " 'mae_mental': 0.15912356972694397,\n",
            " 'mae_physical': 0.1455814242362976,\n",
            " 'r2_mental': 0.15231168270111084,\n",
            " 'r2_physical': 0.2068081498146057,\n",
            " 'rmse_mental': 0.2085588526019593,\n",
            " 'rmse_physical': 0.1919771070174953}\n",
            "[Epoch 12] Avg Training Loss: 0.127197\n",
            "[Epoch 12] Evaluation Metrics:\n",
            "{'loss': 0.07730345634950532,\n",
            " 'mae_mental': 0.15457236766815186,\n",
            " 'mae_physical': 0.14525339007377625,\n",
            " 'r2_mental': 0.1829162836074829,\n",
            " 'r2_physical': 0.21532434225082397,\n",
            " 'rmse_mental': 0.2047593773154711,\n",
            " 'rmse_physical': 0.19094373329152287}\n",
            "[Epoch 13] Avg Training Loss: 0.123371\n",
            "[Epoch 13] Evaluation Metrics:\n",
            "{'loss': 0.07911954913288355,\n",
            " 'mae_mental': 0.15928982198238373,\n",
            " 'mae_physical': 0.14158080518245697,\n",
            " 'r2_mental': 0.1312965750694275,\n",
            " 'r2_physical': 0.2384987473487854,\n",
            " 'rmse_mental': 0.21112821928633596,\n",
            " 'rmse_physical': 0.18810296461886158}\n",
            "[Epoch 14] Avg Training Loss: 0.121772\n",
            "[Epoch 14] Evaluation Metrics:\n",
            "{'loss': 0.07741560176428822,\n",
            " 'mae_mental': 0.15547432005405426,\n",
            " 'mae_physical': 0.15371733903884888,\n",
            " 'r2_mental': 0.20834237337112427,\n",
            " 'r2_physical': 0.18369489908218384,\n",
            " 'rmse_mental': 0.20154834446251954,\n",
            " 'rmse_physical': 0.1947540787947935}\n",
            "[Epoch 15] Avg Training Loss: 0.119540\n",
            "[Epoch 15] Evaluation Metrics:\n",
            "{'loss': 0.08001142295284404,\n",
            " 'mae_mental': 0.15601272881031036,\n",
            " 'mae_physical': 0.14470967650413513,\n",
            " 'r2_mental': 0.13128584623336792,\n",
            " 'r2_physical': 0.22132784128189087,\n",
            " 'rmse_mental': 0.21112952498876925,\n",
            " 'rmse_physical': 0.19021188066405154}\n",
            "[Epoch 16] Avg Training Loss: 0.119264\n",
            "[Epoch 16] Evaluation Metrics:\n",
            "{'loss': 0.07858679153852993,\n",
            " 'mae_mental': 0.1595274806022644,\n",
            " 'mae_physical': 0.14716362953186035,\n",
            " 'r2_mental': 0.13945066928863525,\n",
            " 'r2_physical': 0.23279881477355957,\n",
            " 'rmse_mental': 0.2101350107060642,\n",
            " 'rmse_physical': 0.18880563381846752}\n",
            "[Epoch 17] Avg Training Loss: 0.114758\n",
            "[Epoch 17] Evaluation Metrics:\n",
            "{'loss': 0.07571704530467589,\n",
            " 'mae_mental': 0.15296386182308197,\n",
            " 'mae_physical': 0.1442614048719406,\n",
            " 'r2_mental': 0.19142907857894897,\n",
            " 'r2_physical': 0.24244379997253418,\n",
            " 'rmse_mental': 0.20368994497685278,\n",
            " 'rmse_physical': 0.18761508112083788}\n",
            "[Epoch 18] Avg Training Loss: 0.114378\n",
            "[Epoch 18] Evaluation Metrics:\n",
            "{'loss': 0.07581544284605318,\n",
            " 'mae_mental': 0.157552570104599,\n",
            " 'mae_physical': 0.14639398455619812,\n",
            " 'r2_mental': 0.18665426969528198,\n",
            " 'r2_physical': 0.24391090869903564,\n",
            " 'rmse_mental': 0.20429048509980305,\n",
            " 'rmse_physical': 0.18743333028599082}\n",
            "[Epoch 19] Avg Training Loss: 0.113619\n",
            "[Epoch 19] Evaluation Metrics:\n",
            "{'loss': 0.07891318067494366,\n",
            " 'mae_mental': 0.1568886637687683,\n",
            " 'mae_physical': 0.14859318733215332,\n",
            " 'r2_mental': 0.15604430437088013,\n",
            " 'r2_physical': 0.21128225326538086,\n",
            " 'rmse_mental': 0.2080991750308755,\n",
            " 'rmse_physical': 0.19143490674114652}\n",
            "[Epoch 20] Avg Training Loss: 0.112915\n",
            "[Epoch 20] Evaluation Metrics:\n",
            "{'loss': 0.08001215124709739,\n",
            " 'mae_mental': 0.1615542471408844,\n",
            " 'mae_physical': 0.15391544997692108,\n",
            " 'r2_mental': 0.16826075315475464,\n",
            " 'r2_physical': 0.17079931497573853,\n",
            " 'rmse_mental': 0.2065875408876556,\n",
            " 'rmse_physical': 0.1962863770106689}\n",
            "[Epoch 21] Avg Training Loss: 0.111972\n",
            "[Epoch 21] Evaluation Metrics:\n",
            "{'loss': 0.07648333182765378,\n",
            " 'mae_mental': 0.15035109221935272,\n",
            " 'mae_physical': 0.14526565372943878,\n",
            " 'r2_mental': 0.20209664106369019,\n",
            " 'r2_physical': 0.21388131380081177,\n",
            " 'rmse_mental': 0.20234182570437212,\n",
            " 'rmse_physical': 0.1911192221051978}\n",
            "[Epoch 22] Avg Training Loss: 0.112130\n",
            "[Epoch 22] Evaluation Metrics:\n",
            "{'loss': 0.0757243542207612,\n",
            " 'mae_mental': 0.14953918755054474,\n",
            " 'mae_physical': 0.14908482134342194,\n",
            " 'r2_mental': 0.2286456823348999,\n",
            " 'r2_physical': 0.1991194486618042,\n",
            " 'rmse_mental': 0.19894703867111183,\n",
            " 'rmse_physical': 0.19290531675705322}\n",
            "[Epoch 23] Avg Training Loss: 0.112889\n",
            "[Epoch 23] Evaluation Metrics:\n",
            "{'loss': 0.07545926577101152,\n",
            " 'mae_mental': 0.15198688209056854,\n",
            " 'mae_physical': 0.1492641121149063,\n",
            " 'r2_mental': 0.23977869749069214,\n",
            " 'r2_physical': 0.1908586621284485,\n",
            " 'rmse_mental': 0.1975061159303603,\n",
            " 'rmse_physical': 0.19389763483279202}\n",
            "[Epoch 24] Avg Training Loss: 0.110443\n",
            "[Epoch 24] Evaluation Metrics:\n",
            "{'loss': 0.07608187074462573,\n",
            " 'mae_mental': 0.15648338198661804,\n",
            " 'mae_physical': 0.14629626274108887,\n",
            " 'r2_mental': 0.19699740409851074,\n",
            " 'r2_physical': 0.227617084980011,\n",
            " 'rmse_mental': 0.20298736751177862,\n",
            " 'rmse_physical': 0.1894421623440327}\n",
            "[Epoch 25] Avg Training Loss: 0.108782\n",
            "[Epoch 25] Evaluation Metrics:\n",
            "{'loss': 0.0702340238624149,\n",
            " 'mae_mental': 0.1436748057603836,\n",
            " 'mae_physical': 0.14356212317943573,\n",
            " 'r2_mental': 0.28446751832962036,\n",
            " 'r2_physical': 0.2567562460899353,\n",
            " 'rmse_mental': 0.19161310501406262,\n",
            " 'rmse_physical': 0.18583433069900904}\n",
            "[Epoch 26] Avg Training Loss: 0.106058\n",
            "[Epoch 26] Evaluation Metrics:\n",
            "{'loss': 0.07171196780271,\n",
            " 'mae_mental': 0.14766710996627808,\n",
            " 'mae_physical': 0.14392612874507904,\n",
            " 'r2_mental': 0.2573448419570923,\n",
            " 'r2_physical': 0.2530057430267334,\n",
            " 'rmse_mental': 0.19521092634673906,\n",
            " 'rmse_physical': 0.18630261356518968}\n",
            "[Epoch 27] Avg Training Loss: 0.108285\n",
            "[Epoch 27] Evaluation Metrics:\n",
            "{'loss': 0.07987278855095307,\n",
            " 'mae_mental': 0.15738791227340698,\n",
            " 'mae_physical': 0.14700280129909515,\n",
            " 'r2_mental': 0.16078978776931763,\n",
            " 'r2_physical': 0.1835044026374817,\n",
            " 'rmse_mental': 0.20751328402820865,\n",
            " 'rmse_physical': 0.19477681130526825}\n",
            "[Epoch 28] Avg Training Loss: 0.107292\n",
            "[Epoch 28] Evaluation Metrics:\n",
            "{'loss': 0.07088139849818414,\n",
            " 'mae_mental': 0.14643177390098572,\n",
            " 'mae_physical': 0.14025801420211792,\n",
            " 'r2_mental': 0.28172868490219116,\n",
            " 'r2_physical': 0.2478122115135193,\n",
            " 'rmse_mental': 0.19197947439671686,\n",
            " 'rmse_physical': 0.1869491392763662}\n",
            "[Epoch 29] Avg Training Loss: 0.103535\n",
            "[Epoch 29] Evaluation Metrics:\n",
            "{'loss': 0.07043453792317046,\n",
            " 'mae_mental': 0.14242999255657196,\n",
            " 'mae_physical': 0.14277644455432892,\n",
            " 'r2_mental': 0.28946101665496826,\n",
            " 'r2_physical': 0.24911344051361084,\n",
            " 'rmse_mental': 0.19094332358349606,\n",
            " 'rmse_physical': 0.18678736363745307}\n",
            "[Epoch 30] Avg Training Loss: 0.104498\n",
            "[Epoch 30] Evaluation Metrics:\n",
            "{'loss': 0.0794536592438817,\n",
            " 'mae_mental': 0.15446002781391144,\n",
            " 'mae_physical': 0.15225157141685486,\n",
            " 'r2_mental': 0.21807461977005005,\n",
            " 'r2_physical': 0.130682110786438,\n",
            " 'rmse_mental': 0.2003056385007739,\n",
            " 'rmse_physical': 0.20097851024755242}\n",
            "[Epoch 31] Avg Training Loss: 0.103810\n",
            "[Epoch 31] Evaluation Metrics:\n",
            "{'loss': 0.07426180887139505,\n",
            " 'mae_mental': 0.14810310304164886,\n",
            " 'mae_physical': 0.14563487470149994,\n",
            " 'r2_mental': 0.2534964084625244,\n",
            " 'r2_physical': 0.20422804355621338,\n",
            " 'rmse_mental': 0.1957160690351795,\n",
            " 'rmse_physical': 0.19228908753723298}\n",
            "[Epoch 32] Avg Training Loss: 0.103519\n",
            "[Epoch 32] Evaluation Metrics:\n",
            "{'loss': 0.0690997772746616,\n",
            " 'mae_mental': 0.14449435472488403,\n",
            " 'mae_physical': 0.1424504518508911,\n",
            " 'r2_mental': 0.2913949489593506,\n",
            " 'r2_physical': 0.27474677562713623,\n",
            " 'rmse_mental': 0.19068329380976523,\n",
            " 'rmse_physical': 0.1835714590459987}\n",
            "[Epoch 33] Avg Training Loss: 0.103213\n",
            "[Epoch 33] Evaluation Metrics:\n",
            "{'loss': 0.07508344813767406,\n",
            " 'mae_mental': 0.15114445984363556,\n",
            " 'mae_physical': 0.14656859636306763,\n",
            " 'r2_mental': 0.22643166780471802,\n",
            " 'r2_physical': 0.21444261074066162,\n",
            " 'rmse_mental': 0.1992323534191843,\n",
            " 'rmse_physical': 0.19105097827730796}\n",
            "[Epoch 34] Avg Training Loss: 0.102542\n",
            "[Epoch 34] Evaluation Metrics:\n",
            "{'loss': 0.06702255737036467,\n",
            " 'mae_mental': 0.14349038898944855,\n",
            " 'mae_physical': 0.13524889945983887,\n",
            " 'r2_mental': 0.3161865472793579,\n",
            " 'r2_physical': 0.2943192720413208,\n",
            " 'rmse_mental': 0.18731792868549044,\n",
            " 'rmse_physical': 0.1810774841277503}\n",
            "[Epoch 35] Avg Training Loss: 0.099329\n",
            "[Epoch 35] Evaluation Metrics:\n",
            "{'loss': 0.07680039418240388,\n",
            " 'mae_mental': 0.15633417665958405,\n",
            " 'mae_physical': 0.1390552669763565,\n",
            " 'r2_mental': 0.17053526639938354,\n",
            " 'r2_physical': 0.2471923828125,\n",
            " 'rmse_mental': 0.20630487737871894,\n",
            " 'rmse_physical': 0.1870261403392084}\n",
            "[Epoch 36] Avg Training Loss: 0.100116\n",
            "[Epoch 36] Evaluation Metrics:\n",
            "{'loss': 0.07094515290939146,\n",
            " 'mae_mental': 0.14949606359004974,\n",
            " 'mae_physical': 0.13664649426937103,\n",
            " 'r2_mental': 0.2551288604736328,\n",
            " 'r2_physical': 0.2752712368965149,\n",
            " 'rmse_mental': 0.1955019508877902,\n",
            " 'rmse_physical': 0.18350507745682282}\n",
            "[Epoch 37] Avg Training Loss: 0.097280\n",
            "[Epoch 37] Evaluation Metrics:\n",
            "{'loss': 0.06933996479751335,\n",
            " 'mae_mental': 0.14443160593509674,\n",
            " 'mae_physical': 0.14230571687221527,\n",
            " 'r2_mental': 0.2997599244117737,\n",
            " 'r2_physical': 0.2603766918182373,\n",
            " 'rmse_mental': 0.18955446266243053,\n",
            " 'rmse_physical': 0.1853811728780226}\n",
            "[Epoch 38] Avg Training Loss: 0.100428\n",
            "[Epoch 38] Evaluation Metrics:\n",
            "{'loss': 0.06805915975322326,\n",
            " 'mae_mental': 0.14350925385951996,\n",
            " 'mae_physical': 0.13803783059120178,\n",
            " 'r2_mental': 0.3135293126106262,\n",
            " 'r2_physical': 0.2688068747520447,\n",
            " 'rmse_mental': 0.18768152748002792,\n",
            " 'rmse_physical': 0.18432166491928975}\n",
            "[Epoch 39] Avg Training Loss: 0.101735\n",
            "[Epoch 39] Evaluation Metrics:\n",
            "{'loss': 0.06508177953461806,\n",
            " 'mae_mental': 0.1390063762664795,\n",
            " 'mae_physical': 0.13734076917171478,\n",
            " 'r2_mental': 0.34715670347213745,\n",
            " 'r2_physical': 0.3016285300254822,\n",
            " 'rmse_mental': 0.1830269301778932,\n",
            " 'rmse_physical': 0.1801372683335852}\n",
            "[Epoch 40] Avg Training Loss: 0.096305\n",
            "[Epoch 40] Evaluation Metrics:\n",
            "{'loss': 0.07002444435945815,\n",
            " 'mae_mental': 0.14669665694236755,\n",
            " 'mae_physical': 0.13306567072868347,\n",
            " 'r2_mental': 0.25917840003967285,\n",
            " 'r2_physical': 0.2928723096847534,\n",
            " 'rmse_mental': 0.19496980165826788,\n",
            " 'rmse_physical': 0.18126303897193796}\n",
            "[Epoch 41] Avg Training Loss: 0.095327\n",
            "[Epoch 41] Evaluation Metrics:\n",
            "{'loss': 0.07753335994978745,\n",
            " 'mae_mental': 0.15001407265663147,\n",
            " 'mae_physical': 0.14860844612121582,\n",
            " 'r2_mental': 0.21716827154159546,\n",
            " 'r2_physical': 0.17830556631088257,\n",
            " 'rmse_mental': 0.2004216937841029,\n",
            " 'rmse_physical': 0.19539591915152998}\n",
            "[Epoch 42] Avg Training Loss: 0.093097\n",
            "[Epoch 42] Evaluation Metrics:\n",
            "{'loss': 0.06747012864798307,\n",
            " 'mae_mental': 0.1407911628484726,\n",
            " 'mae_physical': 0.13651062548160553,\n",
            " 'r2_mental': 0.31858694553375244,\n",
            " 'r2_physical': 0.2832460403442383,\n",
            " 'rmse_mental': 0.18698886901065642,\n",
            " 'rmse_physical': 0.1824926471862007}\n",
            "[Epoch 43] Avg Training Loss: 0.093027\n",
            "[Epoch 43] Evaluation Metrics:\n",
            "{'loss': 0.07046255303753747,\n",
            " 'mae_mental': 0.14640165865421295,\n",
            " 'mae_physical': 0.13939382135868073,\n",
            " 'r2_mental': 0.2787785530090332,\n",
            " 'r2_physical': 0.2614818811416626,\n",
            " 'rmse_mental': 0.19237332393270556,\n",
            " 'rmse_physical': 0.18524261424107305}\n",
            "[Epoch 44] Avg Training Loss: 0.094987\n",
            "[Epoch 44] Evaluation Metrics:\n",
            "{'loss': 0.08839960075500938,\n",
            " 'mae_mental': 0.16768592596054077,\n",
            " 'mae_physical': 0.1537010222673416,\n",
            " 'r2_mental': 0.08829867839813232,\n",
            " 'r2_physical': 0.07567298412322998,\n",
            " 'rmse_mental': 0.21629019114313697,\n",
            " 'rmse_physical': 0.20723979271335446}\n",
            "[Epoch 45] Avg Training Loss: 0.096067\n",
            "[Epoch 45] Evaluation Metrics:\n",
            "{'loss': 0.07103028262241019,\n",
            " 'mae_mental': 0.1450444757938385,\n",
            " 'mae_physical': 0.14025118947029114,\n",
            " 'r2_mental': 0.2946814298629761,\n",
            " 'r2_physical': 0.2359289526939392,\n",
            " 'rmse_mental': 0.19024059003511967,\n",
            " 'rmse_physical': 0.18842008465938814}\n",
            "[Epoch 46] Avg Training Loss: 0.092626\n",
            "[Epoch 46] Evaluation Metrics:\n",
            "{'loss': 0.06312724984147483,\n",
            " 'mae_mental': 0.13828587532043457,\n",
            " 'mae_physical': 0.13362732529640198,\n",
            " 'r2_mental': 0.3559395670890808,\n",
            " 'r2_physical': 0.3341732621192932,\n",
            " 'rmse_mental': 0.1817916119530463,\n",
            " 'rmse_physical': 0.17588991409243487}\n",
            "[Epoch 47] Avg Training Loss: 0.092396\n",
            "[Epoch 47] Evaluation Metrics:\n",
            "{'loss': 0.06898312011940612,\n",
            " 'mae_mental': 0.14026974141597748,\n",
            " 'mae_physical': 0.14014437794685364,\n",
            " 'r2_mental': 0.31769394874572754,\n",
            " 'r2_physical': 0.249148428440094,\n",
            " 'rmse_mental': 0.18711135242030483,\n",
            " 'rmse_physical': 0.18678300581897087}\n",
            "[Epoch 48] Avg Training Loss: 0.090797\n",
            "[Epoch 48] Evaluation Metrics:\n",
            "{'loss': 0.06630509698556529,\n",
            " 'mae_mental': 0.14193852245807648,\n",
            " 'mae_physical': 0.1334390789270401,\n",
            " 'r2_mental': 0.30431538820266724,\n",
            " 'r2_physical': 0.32021892070770264,\n",
            " 'rmse_mental': 0.1889368770909617,\n",
            " 'rmse_physical': 0.17772349649191246}\n",
            "[Epoch 49] Avg Training Loss: 0.091130\n",
            "[Epoch 49] Evaluation Metrics:\n",
            "{'loss': 0.06496844404480523,\n",
            " 'mae_mental': 0.1399768590927124,\n",
            " 'mae_physical': 0.1343252807855606,\n",
            " 'r2_mental': 0.3491615653038025,\n",
            " 'r2_physical': 0.30568432807922363,\n",
            " 'rmse_mental': 0.18274567923415144,\n",
            " 'rmse_physical': 0.17961342984965736}\n",
            "[Epoch 50] Avg Training Loss: 0.089256\n",
            "[Epoch 50] Evaluation Metrics:\n",
            "{'loss': 0.06634184355951017,\n",
            " 'mae_mental': 0.13959017395973206,\n",
            " 'mae_physical': 0.13181693851947784,\n",
            " 'r2_mental': 0.3237205147743225,\n",
            " 'r2_physical': 0.30234384536743164,\n",
            " 'rmse_mental': 0.18628317652670676,\n",
            " 'rmse_physical': 0.1800449899404124}\n",
            "[Epoch 51] Avg Training Loss: 0.086798\n",
            "[Epoch 51] Evaluation Metrics:\n",
            "{'loss': 0.06808064360585478,\n",
            " 'mae_mental': 0.14489594101905823,\n",
            " 'mae_physical': 0.13560448586940765,\n",
            " 'r2_mental': 0.2932928204536438,\n",
            " 'r2_physical': 0.29370540380477905,\n",
            " 'rmse_mental': 0.1904277703604418,\n",
            " 'rmse_physical': 0.18115622009900925}\n",
            "[Epoch 52] Avg Training Loss: 0.090376\n",
            "[Epoch 52] Evaluation Metrics:\n",
            "{'loss': 0.07053168625053433,\n",
            " 'mae_mental': 0.14244666695594788,\n",
            " 'mae_physical': 0.137772798538208,\n",
            " 'r2_mental': 0.2913985848426819,\n",
            " 'r2_physical': 0.2533804178237915,\n",
            " 'rmse_mental': 0.19068280539583618,\n",
            " 'rmse_physical': 0.18625588726173378}\n",
            "[Epoch 53] Avg Training Loss: 0.091202\n",
            "[Epoch 53] Evaluation Metrics:\n",
            "{'loss': 0.07356619721071588,\n",
            " 'mae_mental': 0.14660747349262238,\n",
            " 'mae_physical': 0.14388416707515717,\n",
            " 'r2_mental': 0.24786591529846191,\n",
            " 'r2_physical': 0.22899103164672852,\n",
            " 'rmse_mental': 0.19645277000437042,\n",
            " 'rmse_physical': 0.18927360169613827}\n",
            "[Epoch 54] Avg Training Loss: 0.089512\n",
            "[Epoch 54] Evaluation Metrics:\n",
            "{'loss': 0.06262982802258597,\n",
            " 'mae_mental': 0.13632754981517792,\n",
            " 'mae_physical': 0.12710587680339813,\n",
            " 'r2_mental': 0.3487069010734558,\n",
            " 'r2_physical': 0.3572421073913574,\n",
            " 'rmse_mental': 0.18280950405241692,\n",
            " 'rmse_physical': 0.1728160304386644}\n",
            "[Epoch 55] Avg Training Loss: 0.087098\n",
            "[Epoch 55] Evaluation Metrics:\n",
            "{'loss': 0.07789891420139207,\n",
            " 'mae_mental': 0.15085189044475555,\n",
            " 'mae_physical': 0.14994987845420837,\n",
            " 'r2_mental': 0.21904802322387695,\n",
            " 'r2_physical': 0.16450858116149902,\n",
            " 'rmse_mental': 0.20018092778327118,\n",
            " 'rmse_physical': 0.19702952958217293}\n",
            "[Epoch 56] Avg Training Loss: 0.086184\n",
            "[Epoch 56] Evaluation Metrics:\n",
            "{'loss': 0.06901885848492384,\n",
            " 'mae_mental': 0.14215196669101715,\n",
            " 'mae_physical': 0.1346133053302765,\n",
            " 'r2_mental': 0.274016797542572,\n",
            " 'r2_physical': 0.2948915958404541,\n",
            " 'rmse_mental': 0.1930073317286501,\n",
            " 'rmse_physical': 0.18100404452908017}\n",
            "[Epoch 57] Avg Training Loss: 0.088819\n",
            "[Epoch 57] Evaluation Metrics:\n",
            "{'loss': 0.06459179618913266,\n",
            " 'mae_mental': 0.1430964171886444,\n",
            " 'mae_physical': 0.129506453871727,\n",
            " 'r2_mental': 0.33985596895217896,\n",
            " 'r2_physical': 0.32153773307800293,\n",
            " 'rmse_mental': 0.184047483242904,\n",
            " 'rmse_physical': 0.17755101775318388}\n",
            "[Epoch 58] Avg Training Loss: 0.087421\n",
            "[Epoch 58] Evaluation Metrics:\n",
            "{'loss': 0.06626253709610966,\n",
            " 'mae_mental': 0.14073355495929718,\n",
            " 'mae_physical': 0.13368187844753265,\n",
            " 'r2_mental': 0.31316888332366943,\n",
            " 'r2_physical': 0.31306278705596924,\n",
            " 'rmse_mental': 0.18773079615121316,\n",
            " 'rmse_physical': 0.17865650202606773}\n",
            "[Epoch 59] Avg Training Loss: 0.086738\n",
            "[Epoch 59] Evaluation Metrics:\n",
            "{'loss': 0.05990412789914343,\n",
            " 'mae_mental': 0.13503773510456085,\n",
            " 'mae_physical': 0.128567636013031,\n",
            " 'r2_mental': 0.3996450901031494,\n",
            " 'r2_physical': 0.35940808057785034,\n",
            " 'rmse_mental': 0.1755151270656676,\n",
            " 'rmse_physical': 0.17252460635086023}\n",
            "[Epoch 60] Avg Training Loss: 0.087230\n",
            "[Epoch 60] Evaluation Metrics:\n",
            "{'loss': 0.06534168393247658,\n",
            " 'mae_mental': 0.1392332911491394,\n",
            " 'mae_physical': 0.13221947848796844,\n",
            " 'r2_mental': 0.3268110752105713,\n",
            " 'r2_physical': 0.3231305480003357,\n",
            " 'rmse_mental': 0.1858570317444962,\n",
            " 'rmse_physical': 0.17734248585187412}\n",
            "[Epoch 61] Avg Training Loss: 0.089432\n",
            "[Epoch 61] Evaluation Metrics:\n",
            "{'loss': 0.06452710419479343,\n",
            " 'mae_mental': 0.13780368864536285,\n",
            " 'mae_physical': 0.13007627427577972,\n",
            " 'r2_mental': 0.3439597487449646,\n",
            " 'r2_physical': 0.3224206566810608,\n",
            " 'rmse_mental': 0.1834745222875268,\n",
            " 'rmse_physical': 0.1774354453967258}\n",
            "[Epoch 62] Avg Training Loss: 0.086639\n",
            "[Epoch 62] Evaluation Metrics:\n",
            "{'loss': 0.07515473487890428,\n",
            " 'mae_mental': 0.1560211479663849,\n",
            " 'mae_physical': 0.14098569750785828,\n",
            " 'r2_mental': 0.20565491914749146,\n",
            " 'r2_physical': 0.25416100025177,\n",
            " 'rmse_mental': 0.20189015386234316,\n",
            " 'rmse_physical': 0.18615849729010453}\n",
            "[Epoch 63] Avg Training Loss: 0.084683\n",
            "[Epoch 63] Evaluation Metrics:\n",
            "{'loss': 0.05695814246104823,\n",
            " 'mae_mental': 0.1266917735338211,\n",
            " 'mae_physical': 0.12667743861675262,\n",
            " 'r2_mental': 0.44277000427246094,\n",
            " 'r2_physical': 0.37514036893844604,\n",
            " 'rmse_mental': 0.1690938329835687,\n",
            " 'rmse_physical': 0.1703929236588007}\n",
            "[Epoch 64] Avg Training Loss: 0.081296\n",
            "[Epoch 64] Evaluation Metrics:\n",
            "{'loss': 0.06309525296092033,\n",
            " 'mae_mental': 0.1368035078048706,\n",
            " 'mae_physical': 0.1279699057340622,\n",
            " 'r2_mental': 0.34226393699645996,\n",
            " 'r2_physical': 0.3636876344680786,\n",
            " 'rmse_mental': 0.1837115113016001,\n",
            " 'rmse_physical': 0.17194735564588695}\n",
            "[Epoch 65] Avg Training Loss: 0.083724\n",
            "[Epoch 65] Evaluation Metrics:\n",
            "{'loss': 0.09707247155408065,\n",
            " 'mae_mental': 0.1726333200931549,\n",
            " 'mae_physical': 0.17710591852664948,\n",
            " 'r2_mental': 0.031744420528411865,\n",
            " 'r2_physical': -0.01738440990447998,\n",
            " 'rmse_mental': 0.22289766642147757,\n",
            " 'rmse_physical': 0.21742168808934864}\n",
            "[Epoch 66] Avg Training Loss: 0.082516\n",
            "[Epoch 66] Evaluation Metrics:\n",
            "{'loss': 0.0577843968446056,\n",
            " 'mae_mental': 0.13304951786994934,\n",
            " 'mae_physical': 0.12788830697536469,\n",
            " 'r2_mental': 0.41842103004455566,\n",
            " 'r2_physical': 0.3824976086616516,\n",
            " 'rmse_mental': 0.17274873983486647,\n",
            " 'rmse_physical': 0.16938682702251534}\n",
            "[Epoch 67] Avg Training Loss: 0.082303\n",
            "[Epoch 67] Evaluation Metrics:\n",
            "{'loss': 0.05933663642240895,\n",
            " 'mae_mental': 0.1334647536277771,\n",
            " 'mae_physical': 0.134465292096138,\n",
            " 'r2_mental': 0.41664785146713257,\n",
            " 'r2_physical': 0.35545897483825684,\n",
            " 'rmse_mental': 0.17301188323967479,\n",
            " 'rmse_physical': 0.17305558227952506}\n",
            "[Epoch 68] Avg Training Loss: 0.085571\n",
            "[Epoch 68] Evaluation Metrics:\n",
            "{'loss': 0.063247826260825,\n",
            " 'mae_mental': 0.13814155757427216,\n",
            " 'mae_physical': 0.1250823587179184,\n",
            " 'r2_mental': 0.31078946590423584,\n",
            " 'r2_physical': 0.38345104455947876,\n",
            " 'rmse_mental': 0.18805568527040026,\n",
            " 'rmse_physical': 0.16925601283018388}\n",
            "[Epoch 69] Avg Training Loss: 0.082018\n",
            "[Epoch 69] Evaluation Metrics:\n",
            "{'loss': 0.059798647856546774,\n",
            " 'mae_mental': 0.1306081861257553,\n",
            " 'mae_physical': 0.12693342566490173,\n",
            " 'r2_mental': 0.3847242593765259,\n",
            " 'r2_physical': 0.38314300775527954,\n",
            " 'rmse_mental': 0.17768281670905178,\n",
            " 'rmse_physical': 0.1692982883684831}\n",
            "[Epoch 70] Avg Training Loss: 0.081335\n",
            "[Epoch 70] Evaluation Metrics:\n",
            "{'loss': 0.05526168654776282,\n",
            " 'mae_mental': 0.12751197814941406,\n",
            " 'mae_physical': 0.12369009852409363,\n",
            " 'r2_mental': 0.45351070165634155,\n",
            " 'r2_physical': 0.4006389379501343,\n",
            " 'rmse_mental': 0.16745624978810084,\n",
            " 'rmse_physical': 0.16688010709784168}\n",
            "[Epoch 71] Avg Training Loss: 0.079509\n",
            "[Epoch 71] Evaluation Metrics:\n",
            "{'loss': 0.058689912470678486,\n",
            " 'mae_mental': 0.13206849992275238,\n",
            " 'mae_physical': 0.12688708305358887,\n",
            " 'r2_mental': 0.4076927900314331,\n",
            " 'r2_physical': 0.3825857639312744,\n",
            " 'rmse_mental': 0.17433478421482154,\n",
            " 'rmse_physical': 0.1693747360501925}\n",
            "[Epoch 72] Avg Training Loss: 0.079538\n",
            "[Epoch 72] Evaluation Metrics:\n",
            "{'loss': 0.07486107014119625,\n",
            " 'mae_mental': 0.1544903665781021,\n",
            " 'mae_physical': 0.14531248807907104,\n",
            " 'r2_mental': 0.23532545566558838,\n",
            " 'r2_physical': 0.21837776899337769,\n",
            " 'rmse_mental': 0.1980837484790569,\n",
            " 'rmse_physical': 0.19057185417130015}\n",
            "[Epoch 73] Avg Training Loss: 0.079440\n",
            "[Epoch 73] Evaluation Metrics:\n",
            "{'loss': 0.05534917116165161,\n",
            " 'mae_mental': 0.12699271738529205,\n",
            " 'mae_physical': 0.12678514420986176,\n",
            " 'r2_mental': 0.4661509394645691,\n",
            " 'r2_physical': 0.3852778673171997,\n",
            " 'rmse_mental': 0.16550829729986505,\n",
            " 'rmse_physical': 0.16900507470805654}\n",
            "[Epoch 74] Avg Training Loss: 0.076584\n",
            "[Epoch 74] Evaluation Metrics:\n",
            "{'loss': 0.054615637908379235,\n",
            " 'mae_mental': 0.12906639277935028,\n",
            " 'mae_physical': 0.12282595038414001,\n",
            " 'r2_mental': 0.4512457251548767,\n",
            " 'r2_physical': 0.41949719190597534,\n",
            " 'rmse_mental': 0.16780291177398038,\n",
            " 'rmse_physical': 0.16423377678789156}\n",
            "[Epoch 75] Avg Training Loss: 0.078556\n",
            "[Epoch 75] Evaluation Metrics:\n",
            "{'loss': 0.09460717708700234,\n",
            " 'mae_mental': 0.18818001449108124,\n",
            " 'mae_physical': 0.1714186817407608,\n",
            " 'r2_mental': -0.002030611038208008,\n",
            " 'r2_physical': 0.06892544031143188,\n",
            " 'rmse_mental': 0.22675195533620154,\n",
            " 'rmse_physical': 0.20799483675176333}\n",
            "[Epoch 76] Avg Training Loss: 0.086256\n",
            "[Epoch 76] Evaluation Metrics:\n",
            "{'loss': 0.0739326367361678,\n",
            " 'mae_mental': 0.15302880108356476,\n",
            " 'mae_physical': 0.13676956295967102,\n",
            " 'r2_mental': 0.21421420574188232,\n",
            " 'r2_physical': 0.27157413959503174,\n",
            " 'rmse_mental': 0.20079949552111212,\n",
            " 'rmse_physical': 0.1839725360989439}\n",
            "[Epoch 77] Avg Training Loss: 0.078038\n",
            "[Epoch 77] Evaluation Metrics:\n",
            "{'loss': 0.0719969090488222,\n",
            " 'mae_mental': 0.15485647320747375,\n",
            " 'mae_physical': 0.14313116669654846,\n",
            " 'r2_mental': 0.24483829736709595,\n",
            " 'r2_physical': 0.2707363963127136,\n",
            " 'rmse_mental': 0.19684777525719416,\n",
            " 'rmse_physical': 0.18407829746335075}\n",
            "[Epoch 78] Avg Training Loss: 0.080134\n",
            "[Epoch 78] Evaluation Metrics:\n",
            "{'loss': 0.059826843492272824,\n",
            " 'mae_mental': 0.13161754608154297,\n",
            " 'mae_physical': 0.12229032069444656,\n",
            " 'r2_mental': 0.3687490224838257,\n",
            " 'r2_physical': 0.40555548667907715,\n",
            " 'rmse_mental': 0.1799747410244197,\n",
            " 'rmse_physical': 0.1661942474735658}\n",
            "[Epoch 79] Avg Training Loss: 0.073822\n",
            "[Epoch 79] Evaluation Metrics:\n",
            "{'loss': 0.06511391026692258,\n",
            " 'mae_mental': 0.13891710340976715,\n",
            " 'mae_physical': 0.14544127881526947,\n",
            " 'r2_mental': 0.3793891668319702,\n",
            " 'r2_physical': 0.26469147205352783,\n",
            " 'rmse_mental': 0.17845149568228608,\n",
            " 'rmse_physical': 0.18483964755873936}\n",
            "[Epoch 80] Avg Training Loss: 0.073976\n",
            "[Epoch 80] Evaluation Metrics:\n",
            "{'loss': 0.06558012175891134,\n",
            " 'mae_mental': 0.1367477923631668,\n",
            " 'mae_physical': 0.13361486792564392,\n",
            " 'r2_mental': 0.3372201919555664,\n",
            " 'r2_physical': 0.30778050422668457,\n",
            " 'rmse_mental': 0.18441454051221853,\n",
            " 'rmse_physical': 0.1793420934067195}\n",
            "\n",
            "=== Full Metrics History (All Epochs) ===\n",
            "{'loss': [(1, 0.0962678889433543),\n",
            "          (2, 0.08527866311164366),\n",
            "          (3, 0.08531244357840882),\n",
            "          (4, 0.0903224246059027),\n",
            "          (5, 0.0835578650650051),\n",
            "          (6, 0.08390226163383988),\n",
            "          (7, 0.08209170173439714),\n",
            "          (8, 0.08132622960127062),\n",
            "          (9, 0.08152533984846538),\n",
            "          (10, 0.07778032424135341),\n",
            "          (11, 0.07922727035151587),\n",
            "          (12, 0.07730345634950532),\n",
            "          (13, 0.07911954913288355),\n",
            "          (14, 0.07741560176428822),\n",
            "          (15, 0.08001142295284404),\n",
            "          (16, 0.07858679153852993),\n",
            "          (17, 0.07571704530467589),\n",
            "          (18, 0.07581544284605318),\n",
            "          (19, 0.07891318067494366),\n",
            "          (20, 0.08001215124709739),\n",
            "          (21, 0.07648333182765378),\n",
            "          (22, 0.0757243542207612),\n",
            "          (23, 0.07545926577101152),\n",
            "          (24, 0.07608187074462573),\n",
            "          (25, 0.0702340238624149),\n",
            "          (26, 0.07171196780271),\n",
            "          (27, 0.07987278855095307),\n",
            "          (28, 0.07088139849818414),\n",
            "          (29, 0.07043453792317046),\n",
            "          (30, 0.0794536592438817),\n",
            "          (31, 0.07426180887139505),\n",
            "          (32, 0.0690997772746616),\n",
            "          (33, 0.07508344813767406),\n",
            "          (34, 0.06702255737036467),\n",
            "          (35, 0.07680039418240388),\n",
            "          (36, 0.07094515290939146),\n",
            "          (37, 0.06933996479751335),\n",
            "          (38, 0.06805915975322326),\n",
            "          (39, 0.06508177953461806),\n",
            "          (40, 0.07002444435945815),\n",
            "          (41, 0.07753335994978745),\n",
            "          (42, 0.06747012864798307),\n",
            "          (43, 0.07046255303753747),\n",
            "          (44, 0.08839960075500938),\n",
            "          (45, 0.07103028262241019),\n",
            "          (46, 0.06312724984147483),\n",
            "          (47, 0.06898312011940612),\n",
            "          (48, 0.06630509698556529),\n",
            "          (49, 0.06496844404480523),\n",
            "          (50, 0.06634184355951017),\n",
            "          (51, 0.06808064360585478),\n",
            "          (52, 0.07053168625053433),\n",
            "          (53, 0.07356619721071588),\n",
            "          (54, 0.06262982802258597),\n",
            "          (55, 0.07789891420139207),\n",
            "          (56, 0.06901885848492384),\n",
            "          (57, 0.06459179618913266),\n",
            "          (58, 0.06626253709610966),\n",
            "          (59, 0.05990412789914343),\n",
            "          (60, 0.06534168393247658),\n",
            "          (61, 0.06452710419479343),\n",
            "          (62, 0.07515473487890428),\n",
            "          (63, 0.05695814246104823),\n",
            "          (64, 0.06309525296092033),\n",
            "          (65, 0.09707247155408065),\n",
            "          (66, 0.0577843968446056),\n",
            "          (67, 0.05933663642240895),\n",
            "          (68, 0.063247826260825),\n",
            "          (69, 0.059798647856546774),\n",
            "          (70, 0.05526168654776282),\n",
            "          (71, 0.058689912470678486),\n",
            "          (72, 0.07486107014119625),\n",
            "          (73, 0.05534917116165161),\n",
            "          (74, 0.054615637908379235),\n",
            "          (75, 0.09460717708700234),\n",
            "          (76, 0.0739326367361678),\n",
            "          (77, 0.0719969090488222),\n",
            "          (78, 0.059826843492272824),\n",
            "          (79, 0.06511391026692258),\n",
            "          (80, 0.06558012175891134)],\n",
            " 'mae_mental': [(1, 0.17874468863010406),\n",
            "                (2, 0.16470257937908173),\n",
            "                (3, 0.16737863421440125),\n",
            "                (4, 0.16009025275707245),\n",
            "                (5, 0.1654800921678543),\n",
            "                (6, 0.15994474291801453),\n",
            "                (7, 0.15826791524887085),\n",
            "                (8, 0.16115480661392212),\n",
            "                (9, 0.16368399560451508),\n",
            "                (10, 0.15797163546085358),\n",
            "                (11, 0.15912356972694397),\n",
            "                (12, 0.15457236766815186),\n",
            "                (13, 0.15928982198238373),\n",
            "                (14, 0.15547432005405426),\n",
            "                (15, 0.15601272881031036),\n",
            "                (16, 0.1595274806022644),\n",
            "                (17, 0.15296386182308197),\n",
            "                (18, 0.157552570104599),\n",
            "                (19, 0.1568886637687683),\n",
            "                (20, 0.1615542471408844),\n",
            "                (21, 0.15035109221935272),\n",
            "                (22, 0.14953918755054474),\n",
            "                (23, 0.15198688209056854),\n",
            "                (24, 0.15648338198661804),\n",
            "                (25, 0.1436748057603836),\n",
            "                (26, 0.14766710996627808),\n",
            "                (27, 0.15738791227340698),\n",
            "                (28, 0.14643177390098572),\n",
            "                (29, 0.14242999255657196),\n",
            "                (30, 0.15446002781391144),\n",
            "                (31, 0.14810310304164886),\n",
            "                (32, 0.14449435472488403),\n",
            "                (33, 0.15114445984363556),\n",
            "                (34, 0.14349038898944855),\n",
            "                (35, 0.15633417665958405),\n",
            "                (36, 0.14949606359004974),\n",
            "                (37, 0.14443160593509674),\n",
            "                (38, 0.14350925385951996),\n",
            "                (39, 0.1390063762664795),\n",
            "                (40, 0.14669665694236755),\n",
            "                (41, 0.15001407265663147),\n",
            "                (42, 0.1407911628484726),\n",
            "                (43, 0.14640165865421295),\n",
            "                (44, 0.16768592596054077),\n",
            "                (45, 0.1450444757938385),\n",
            "                (46, 0.13828587532043457),\n",
            "                (47, 0.14026974141597748),\n",
            "                (48, 0.14193852245807648),\n",
            "                (49, 0.1399768590927124),\n",
            "                (50, 0.13959017395973206),\n",
            "                (51, 0.14489594101905823),\n",
            "                (52, 0.14244666695594788),\n",
            "                (53, 0.14660747349262238),\n",
            "                (54, 0.13632754981517792),\n",
            "                (55, 0.15085189044475555),\n",
            "                (56, 0.14215196669101715),\n",
            "                (57, 0.1430964171886444),\n",
            "                (58, 0.14073355495929718),\n",
            "                (59, 0.13503773510456085),\n",
            "                (60, 0.1392332911491394),\n",
            "                (61, 0.13780368864536285),\n",
            "                (62, 0.1560211479663849),\n",
            "                (63, 0.1266917735338211),\n",
            "                (64, 0.1368035078048706),\n",
            "                (65, 0.1726333200931549),\n",
            "                (66, 0.13304951786994934),\n",
            "                (67, 0.1334647536277771),\n",
            "                (68, 0.13814155757427216),\n",
            "                (69, 0.1306081861257553),\n",
            "                (70, 0.12751197814941406),\n",
            "                (71, 0.13206849992275238),\n",
            "                (72, 0.1544903665781021),\n",
            "                (73, 0.12699271738529205),\n",
            "                (74, 0.12906639277935028),\n",
            "                (75, 0.18818001449108124),\n",
            "                (76, 0.15302880108356476),\n",
            "                (77, 0.15485647320747375),\n",
            "                (78, 0.13161754608154297),\n",
            "                (79, 0.13891710340976715),\n",
            "                (80, 0.1367477923631668)],\n",
            " 'mae_physical': [(1, 0.17359048128128052),\n",
            "                  (2, 0.156401127576828),\n",
            "                  (3, 0.15157687664031982),\n",
            "                  (4, 0.1702238917350769),\n",
            "                  (5, 0.1484343707561493),\n",
            "                  (6, 0.14963386952877045),\n",
            "                  (7, 0.16020362079143524),\n",
            "                  (8, 0.15279334783554077),\n",
            "                  (9, 0.1544286012649536),\n",
            "                  (10, 0.14560380578041077),\n",
            "                  (11, 0.1455814242362976),\n",
            "                  (12, 0.14525339007377625),\n",
            "                  (13, 0.14158080518245697),\n",
            "                  (14, 0.15371733903884888),\n",
            "                  (15, 0.14470967650413513),\n",
            "                  (16, 0.14716362953186035),\n",
            "                  (17, 0.1442614048719406),\n",
            "                  (18, 0.14639398455619812),\n",
            "                  (19, 0.14859318733215332),\n",
            "                  (20, 0.15391544997692108),\n",
            "                  (21, 0.14526565372943878),\n",
            "                  (22, 0.14908482134342194),\n",
            "                  (23, 0.1492641121149063),\n",
            "                  (24, 0.14629626274108887),\n",
            "                  (25, 0.14356212317943573),\n",
            "                  (26, 0.14392612874507904),\n",
            "                  (27, 0.14700280129909515),\n",
            "                  (28, 0.14025801420211792),\n",
            "                  (29, 0.14277644455432892),\n",
            "                  (30, 0.15225157141685486),\n",
            "                  (31, 0.14563487470149994),\n",
            "                  (32, 0.1424504518508911),\n",
            "                  (33, 0.14656859636306763),\n",
            "                  (34, 0.13524889945983887),\n",
            "                  (35, 0.1390552669763565),\n",
            "                  (36, 0.13664649426937103),\n",
            "                  (37, 0.14230571687221527),\n",
            "                  (38, 0.13803783059120178),\n",
            "                  (39, 0.13734076917171478),\n",
            "                  (40, 0.13306567072868347),\n",
            "                  (41, 0.14860844612121582),\n",
            "                  (42, 0.13651062548160553),\n",
            "                  (43, 0.13939382135868073),\n",
            "                  (44, 0.1537010222673416),\n",
            "                  (45, 0.14025118947029114),\n",
            "                  (46, 0.13362732529640198),\n",
            "                  (47, 0.14014437794685364),\n",
            "                  (48, 0.1334390789270401),\n",
            "                  (49, 0.1343252807855606),\n",
            "                  (50, 0.13181693851947784),\n",
            "                  (51, 0.13560448586940765),\n",
            "                  (52, 0.137772798538208),\n",
            "                  (53, 0.14388416707515717),\n",
            "                  (54, 0.12710587680339813),\n",
            "                  (55, 0.14994987845420837),\n",
            "                  (56, 0.1346133053302765),\n",
            "                  (57, 0.129506453871727),\n",
            "                  (58, 0.13368187844753265),\n",
            "                  (59, 0.128567636013031),\n",
            "                  (60, 0.13221947848796844),\n",
            "                  (61, 0.13007627427577972),\n",
            "                  (62, 0.14098569750785828),\n",
            "                  (63, 0.12667743861675262),\n",
            "                  (64, 0.1279699057340622),\n",
            "                  (65, 0.17710591852664948),\n",
            "                  (66, 0.12788830697536469),\n",
            "                  (67, 0.134465292096138),\n",
            "                  (68, 0.1250823587179184),\n",
            "                  (69, 0.12693342566490173),\n",
            "                  (70, 0.12369009852409363),\n",
            "                  (71, 0.12688708305358887),\n",
            "                  (72, 0.14531248807907104),\n",
            "                  (73, 0.12678514420986176),\n",
            "                  (74, 0.12282595038414001),\n",
            "                  (75, 0.1714186817407608),\n",
            "                  (76, 0.13676956295967102),\n",
            "                  (77, 0.14313116669654846),\n",
            "                  (78, 0.12229032069444656),\n",
            "                  (79, 0.14544127881526947),\n",
            "                  (80, 0.13361486792564392)],\n",
            " 'r2_mental': [(1, 0.0024542808532714844),\n",
            "               (2, 0.10842150449752808),\n",
            "               (3, 0.0968400239944458),\n",
            "               (4, 0.11123985052108765),\n",
            "               (5, 0.10616880655288696),\n",
            "               (6, 0.12646937370300293),\n",
            "               (7, 0.16512686014175415),\n",
            "               (8, 0.14767855405807495),\n",
            "               (9, 0.14074355363845825),\n",
            "               (10, 0.17488133907318115),\n",
            "               (11, 0.15231168270111084),\n",
            "               (12, 0.1829162836074829),\n",
            "               (13, 0.1312965750694275),\n",
            "               (14, 0.20834237337112427),\n",
            "               (15, 0.13128584623336792),\n",
            "               (16, 0.13945066928863525),\n",
            "               (17, 0.19142907857894897),\n",
            "               (18, 0.18665426969528198),\n",
            "               (19, 0.15604430437088013),\n",
            "               (20, 0.16826075315475464),\n",
            "               (21, 0.20209664106369019),\n",
            "               (22, 0.2286456823348999),\n",
            "               (23, 0.23977869749069214),\n",
            "               (24, 0.19699740409851074),\n",
            "               (25, 0.28446751832962036),\n",
            "               (26, 0.2573448419570923),\n",
            "               (27, 0.16078978776931763),\n",
            "               (28, 0.28172868490219116),\n",
            "               (29, 0.28946101665496826),\n",
            "               (30, 0.21807461977005005),\n",
            "               (31, 0.2534964084625244),\n",
            "               (32, 0.2913949489593506),\n",
            "               (33, 0.22643166780471802),\n",
            "               (34, 0.3161865472793579),\n",
            "               (35, 0.17053526639938354),\n",
            "               (36, 0.2551288604736328),\n",
            "               (37, 0.2997599244117737),\n",
            "               (38, 0.3135293126106262),\n",
            "               (39, 0.34715670347213745),\n",
            "               (40, 0.25917840003967285),\n",
            "               (41, 0.21716827154159546),\n",
            "               (42, 0.31858694553375244),\n",
            "               (43, 0.2787785530090332),\n",
            "               (44, 0.08829867839813232),\n",
            "               (45, 0.2946814298629761),\n",
            "               (46, 0.3559395670890808),\n",
            "               (47, 0.31769394874572754),\n",
            "               (48, 0.30431538820266724),\n",
            "               (49, 0.3491615653038025),\n",
            "               (50, 0.3237205147743225),\n",
            "               (51, 0.2932928204536438),\n",
            "               (52, 0.2913985848426819),\n",
            "               (53, 0.24786591529846191),\n",
            "               (54, 0.3487069010734558),\n",
            "               (55, 0.21904802322387695),\n",
            "               (56, 0.274016797542572),\n",
            "               (57, 0.33985596895217896),\n",
            "               (58, 0.31316888332366943),\n",
            "               (59, 0.3996450901031494),\n",
            "               (60, 0.3268110752105713),\n",
            "               (61, 0.3439597487449646),\n",
            "               (62, 0.20565491914749146),\n",
            "               (63, 0.44277000427246094),\n",
            "               (64, 0.34226393699645996),\n",
            "               (65, 0.031744420528411865),\n",
            "               (66, 0.41842103004455566),\n",
            "               (67, 0.41664785146713257),\n",
            "               (68, 0.31078946590423584),\n",
            "               (69, 0.3847242593765259),\n",
            "               (70, 0.45351070165634155),\n",
            "               (71, 0.4076927900314331),\n",
            "               (72, 0.23532545566558838),\n",
            "               (73, 0.4661509394645691),\n",
            "               (74, 0.4512457251548767),\n",
            "               (75, -0.002030611038208008),\n",
            "               (76, 0.21421420574188232),\n",
            "               (77, 0.24483829736709595),\n",
            "               (78, 0.3687490224838257),\n",
            "               (79, 0.3793891668319702),\n",
            "               (80, 0.3372201919555664)],\n",
            " 'r2_physical': [(1, 0.004254043102264404),\n",
            "                 (2, 0.1252657175064087),\n",
            "                 (3, 0.1325015425682068),\n",
            "                 (4, 0.015928983688354492),\n",
            "                 (5, 0.16274350881576538),\n",
            "                 (6, 0.13672268390655518),\n",
            "                 (7, 0.13204288482666016),\n",
            "                 (8, 0.16552555561065674),\n",
            "                 (9, 0.16822856664657593),\n",
            "                 (10, 0.21401488780975342),\n",
            "                 (11, 0.2068081498146057),\n",
            "                 (12, 0.21532434225082397),\n",
            "                 (13, 0.2384987473487854),\n",
            "                 (14, 0.18369489908218384),\n",
            "                 (15, 0.22132784128189087),\n",
            "                 (16, 0.23279881477355957),\n",
            "                 (17, 0.24244379997253418),\n",
            "                 (18, 0.24391090869903564),\n",
            "                 (19, 0.21128225326538086),\n",
            "                 (20, 0.17079931497573853),\n",
            "                 (21, 0.21388131380081177),\n",
            "                 (22, 0.1991194486618042),\n",
            "                 (23, 0.1908586621284485),\n",
            "                 (24, 0.227617084980011),\n",
            "                 (25, 0.2567562460899353),\n",
            "                 (26, 0.2530057430267334),\n",
            "                 (27, 0.1835044026374817),\n",
            "                 (28, 0.2478122115135193),\n",
            "                 (29, 0.24911344051361084),\n",
            "                 (30, 0.130682110786438),\n",
            "                 (31, 0.20422804355621338),\n",
            "                 (32, 0.27474677562713623),\n",
            "                 (33, 0.21444261074066162),\n",
            "                 (34, 0.2943192720413208),\n",
            "                 (35, 0.2471923828125),\n",
            "                 (36, 0.2752712368965149),\n",
            "                 (37, 0.2603766918182373),\n",
            "                 (38, 0.2688068747520447),\n",
            "                 (39, 0.3016285300254822),\n",
            "                 (40, 0.2928723096847534),\n",
            "                 (41, 0.17830556631088257),\n",
            "                 (42, 0.2832460403442383),\n",
            "                 (43, 0.2614818811416626),\n",
            "                 (44, 0.07567298412322998),\n",
            "                 (45, 0.2359289526939392),\n",
            "                 (46, 0.3341732621192932),\n",
            "                 (47, 0.249148428440094),\n",
            "                 (48, 0.32021892070770264),\n",
            "                 (49, 0.30568432807922363),\n",
            "                 (50, 0.30234384536743164),\n",
            "                 (51, 0.29370540380477905),\n",
            "                 (52, 0.2533804178237915),\n",
            "                 (53, 0.22899103164672852),\n",
            "                 (54, 0.3572421073913574),\n",
            "                 (55, 0.16450858116149902),\n",
            "                 (56, 0.2948915958404541),\n",
            "                 (57, 0.32153773307800293),\n",
            "                 (58, 0.31306278705596924),\n",
            "                 (59, 0.35940808057785034),\n",
            "                 (60, 0.3231305480003357),\n",
            "                 (61, 0.3224206566810608),\n",
            "                 (62, 0.25416100025177),\n",
            "                 (63, 0.37514036893844604),\n",
            "                 (64, 0.3636876344680786),\n",
            "                 (65, -0.01738440990447998),\n",
            "                 (66, 0.3824976086616516),\n",
            "                 (67, 0.35545897483825684),\n",
            "                 (68, 0.38345104455947876),\n",
            "                 (69, 0.38314300775527954),\n",
            "                 (70, 0.4006389379501343),\n",
            "                 (71, 0.3825857639312744),\n",
            "                 (72, 0.21837776899337769),\n",
            "                 (73, 0.3852778673171997),\n",
            "                 (74, 0.41949719190597534),\n",
            "                 (75, 0.06892544031143188),\n",
            "                 (76, 0.27157413959503174),\n",
            "                 (77, 0.2707363963127136),\n",
            "                 (78, 0.40555548667907715),\n",
            "                 (79, 0.26469147205352783),\n",
            "                 (80, 0.30778050422668457)],\n",
            " 'rmse_mental': [(1, 0.2262439296761638),\n",
            "                 (2, 0.21388992641573712),\n",
            "                 (3, 0.21527463879057138),\n",
            "                 (4, 0.21355159763566076),\n",
            "                 (5, 0.21415996107935328),\n",
            "                 (6, 0.2117140040974236),\n",
            "                 (7, 0.20697637046540404),\n",
            "                 (8, 0.20912802741292885),\n",
            "                 (9, 0.20997710054861388),\n",
            "                 (10, 0.20576368818893054),\n",
            "                 (11, 0.2085588526019593),\n",
            "                 (12, 0.2047593773154711),\n",
            "                 (13, 0.21112821928633596),\n",
            "                 (14, 0.20154834446251954),\n",
            "                 (15, 0.21112952498876925),\n",
            "                 (16, 0.2101350107060642),\n",
            "                 (17, 0.20368994497685278),\n",
            "                 (18, 0.20429048509980305),\n",
            "                 (19, 0.2080991750308755),\n",
            "                 (20, 0.2065875408876556),\n",
            "                 (21, 0.20234182570437212),\n",
            "                 (22, 0.19894703867111183),\n",
            "                 (23, 0.1975061159303603),\n",
            "                 (24, 0.20298736751177862),\n",
            "                 (25, 0.19161310501406262),\n",
            "                 (26, 0.19521092634673906),\n",
            "                 (27, 0.20751328402820865),\n",
            "                 (28, 0.19197947439671686),\n",
            "                 (29, 0.19094332358349606),\n",
            "                 (30, 0.2003056385007739),\n",
            "                 (31, 0.1957160690351795),\n",
            "                 (32, 0.19068329380976523),\n",
            "                 (33, 0.1992323534191843),\n",
            "                 (34, 0.18731792868549044),\n",
            "                 (35, 0.20630487737871894),\n",
            "                 (36, 0.1955019508877902),\n",
            "                 (37, 0.18955446266243053),\n",
            "                 (38, 0.18768152748002792),\n",
            "                 (39, 0.1830269301778932),\n",
            "                 (40, 0.19496980165826788),\n",
            "                 (41, 0.2004216937841029),\n",
            "                 (42, 0.18698886901065642),\n",
            "                 (43, 0.19237332393270556),\n",
            "                 (44, 0.21629019114313697),\n",
            "                 (45, 0.19024059003511967),\n",
            "                 (46, 0.1817916119530463),\n",
            "                 (47, 0.18711135242030483),\n",
            "                 (48, 0.1889368770909617),\n",
            "                 (49, 0.18274567923415144),\n",
            "                 (50, 0.18628317652670676),\n",
            "                 (51, 0.1904277703604418),\n",
            "                 (52, 0.19068280539583618),\n",
            "                 (53, 0.19645277000437042),\n",
            "                 (54, 0.18280950405241692),\n",
            "                 (55, 0.20018092778327118),\n",
            "                 (56, 0.1930073317286501),\n",
            "                 (57, 0.184047483242904),\n",
            "                 (58, 0.18773079615121316),\n",
            "                 (59, 0.1755151270656676),\n",
            "                 (60, 0.1858570317444962),\n",
            "                 (61, 0.1834745222875268),\n",
            "                 (62, 0.20189015386234316),\n",
            "                 (63, 0.1690938329835687),\n",
            "                 (64, 0.1837115113016001),\n",
            "                 (65, 0.22289766642147757),\n",
            "                 (66, 0.17274873983486647),\n",
            "                 (67, 0.17301188323967479),\n",
            "                 (68, 0.18805568527040026),\n",
            "                 (69, 0.17768281670905178),\n",
            "                 (70, 0.16745624978810084),\n",
            "                 (71, 0.17433478421482154),\n",
            "                 (72, 0.1980837484790569),\n",
            "                 (73, 0.16550829729986505),\n",
            "                 (74, 0.16780291177398038),\n",
            "                 (75, 0.22675195533620154),\n",
            "                 (76, 0.20079949552111212),\n",
            "                 (77, 0.19684777525719416),\n",
            "                 (78, 0.1799747410244197),\n",
            "                 (79, 0.17845149568228608),\n",
            "                 (80, 0.18441454051221853)],\n",
            " 'rmse_physical': [(1, 0.21509712190768646),\n",
            "                   (2, 0.20160365756431362),\n",
            "                   (3, 0.20076808403872995),\n",
            "                   (4, 0.21383240821961108),\n",
            "                   (5, 0.1972375415413937),\n",
            "                   (6, 0.2002790322516534),\n",
            "                   (7, 0.20082115415060245),\n",
            "                   (8, 0.19690957371325643),\n",
            "                   (9, 0.1965904010444407),\n",
            "                   (10, 0.19110299435055692),\n",
            "                   (11, 0.1919771070174953),\n",
            "                   (12, 0.19094373329152287),\n",
            "                   (13, 0.18810296461886158),\n",
            "                   (14, 0.1947540787947935),\n",
            "                   (15, 0.19021188066405154),\n",
            "                   (16, 0.18880563381846752),\n",
            "                   (17, 0.18761508112083788),\n",
            "                   (18, 0.18743333028599082),\n",
            "                   (19, 0.19143490674114652),\n",
            "                   (20, 0.1962863770106689),\n",
            "                   (21, 0.1911192221051978),\n",
            "                   (22, 0.19290531675705322),\n",
            "                   (23, 0.19389763483279202),\n",
            "                   (24, 0.1894421623440327),\n",
            "                   (25, 0.18583433069900904),\n",
            "                   (26, 0.18630261356518968),\n",
            "                   (27, 0.19477681130526825),\n",
            "                   (28, 0.1869491392763662),\n",
            "                   (29, 0.18678736363745307),\n",
            "                   (30, 0.20097851024755242),\n",
            "                   (31, 0.19228908753723298),\n",
            "                   (32, 0.1835714590459987),\n",
            "                   (33, 0.19105097827730796),\n",
            "                   (34, 0.1810774841277503),\n",
            "                   (35, 0.1870261403392084),\n",
            "                   (36, 0.18350507745682282),\n",
            "                   (37, 0.1853811728780226),\n",
            "                   (38, 0.18432166491928975),\n",
            "                   (39, 0.1801372683335852),\n",
            "                   (40, 0.18126303897193796),\n",
            "                   (41, 0.19539591915152998),\n",
            "                   (42, 0.1824926471862007),\n",
            "                   (43, 0.18524261424107305),\n",
            "                   (44, 0.20723979271335446),\n",
            "                   (45, 0.18842008465938814),\n",
            "                   (46, 0.17588991409243487),\n",
            "                   (47, 0.18678300581897087),\n",
            "                   (48, 0.17772349649191246),\n",
            "                   (49, 0.17961342984965736),\n",
            "                   (50, 0.1800449899404124),\n",
            "                   (51, 0.18115622009900925),\n",
            "                   (52, 0.18625588726173378),\n",
            "                   (53, 0.18927360169613827),\n",
            "                   (54, 0.1728160304386644),\n",
            "                   (55, 0.19702952958217293),\n",
            "                   (56, 0.18100404452908017),\n",
            "                   (57, 0.17755101775318388),\n",
            "                   (58, 0.17865650202606773),\n",
            "                   (59, 0.17252460635086023),\n",
            "                   (60, 0.17734248585187412),\n",
            "                   (61, 0.1774354453967258),\n",
            "                   (62, 0.18615849729010453),\n",
            "                   (63, 0.1703929236588007),\n",
            "                   (64, 0.17194735564588695),\n",
            "                   (65, 0.21742168808934864),\n",
            "                   (66, 0.16938682702251534),\n",
            "                   (67, 0.17305558227952506),\n",
            "                   (68, 0.16925601283018388),\n",
            "                   (69, 0.1692982883684831),\n",
            "                   (70, 0.16688010709784168),\n",
            "                   (71, 0.1693747360501925),\n",
            "                   (72, 0.19057185417130015),\n",
            "                   (73, 0.16900507470805654),\n",
            "                   (74, 0.16423377678789156),\n",
            "                   (75, 0.20799483675176333),\n",
            "                   (76, 0.1839725360989439),\n",
            "                   (77, 0.18407829746335075),\n",
            "                   (78, 0.1661942474735658),\n",
            "                   (79, 0.18483964755873936),\n",
            "                   (80, 0.1793420934067195)]}\n",
            "\n",
            "--- Running Final Evaluation on Test Set ---\n",
            "\n",
            "=== Predictions vs Ground Truth (first test batch - FINAL EVAL) ===\n",
            "Sample 0: True phys=0.187, True ment=0.183 | Pred phys=0.142, Pred ment=0.122\n",
            "Sample 1: True phys=0.187, True ment=0.183 | Pred phys=0.187, Pred ment=0.036\n",
            "Sample 2: True phys=0.187, True ment=0.183 | Pred phys=-0.118, Pred ment=0.033\n",
            "Sample 3: True phys=0.188, True ment=0.183 | Pred phys=0.104, Pred ment=0.363\n",
            "Sample 4: True phys=0.200, True ment=0.180 | Pred phys=0.181, Pred ment=0.299\n",
            "Sample 5: True phys=0.208, True ment=0.177 | Pred phys=0.151, Pred ment=0.279\n",
            "Sample 6: True phys=0.261, True ment=0.161 | Pred phys=0.369, Pred ment=0.310\n",
            "Sample 7: True phys=0.265, True ment=0.160 | Pred phys=0.366, Pred ment=0.314\n",
            "Sample 8: True phys=0.269, True ment=0.158 | Pred phys=0.361, Pred ment=0.304\n",
            "Sample 9: True phys=0.278, True ment=0.156 | Pred phys=0.359, Pred ment=0.298\n",
            "\n",
            "=== Final Test Performance (after all epochs) ===\n",
            "Test Regression Loss (avg): 0.065580\n",
            "Physical Fatigue - RMSE: 0.179342, MAE: 0.133615, R²: 0.307780\n",
            "Mental Fatigue   - RMSE: 0.184415, MAE: 0.136748, R²: 0.337220\n",
            "\n",
            "=== Final Metrics Dictionary (after all epochs) ===\n",
            "{'loss': 0.06558012175891134,\n",
            " 'mae_mental': 0.13674778888384,\n",
            " 'mae_physical': 0.1336148749889991,\n",
            " 'r2_mental': 0.3372201997398264,\n",
            " 'r2_physical': 0.30778048850934303,\n",
            " 'rmse_mental': 0.18441453694732923,\n",
            " 'rmse_physical': 0.17934210015346874}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import pprint # Import pprint for nice dictionary printing\n",
        "from collections import defaultdict # To easily append metrics\n",
        "\n",
        "# =========================\n",
        "# Config / Debug\n",
        "# =========================\n",
        "DEBUG_ATTENTION = False  # set True to print domain scaling + attention weights for the first batch\n",
        "\n",
        "# ========== Dataset ==========\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each __getitem__ = one timestep.\n",
        "    - Converts session to int and uses it as a numeric feature (optionally can embed later).\n",
        "    - Computes elapsed_time within each (person, session) from window_start.\n",
        "    - Returns modal tensors, domain_disc placeholder, session_feat, elapsed_time, and targets.\n",
        "    \"\"\"\n",
        "    def __init__(self, df, session_map=None, compute_elapsed=True):\n",
        "        df = df.copy()\n",
        "        # ensure correct types\n",
        "        # Convert only if not already int/float, handle potential errors\n",
        "        df['session'] = pd.to_numeric(df['session'], errors='coerce').fillna(0).astype(int)\n",
        "        df['person'] = pd.to_numeric(df['person'], errors='coerce').fillna(0).astype(int)\n",
        "        # parse window_start as datetime\n",
        "        df['window_start'] = pd.to_datetime(df['window_start'], errors='coerce')\n",
        "        df.dropna(subset=['window_start'], inplace=True) # Drop rows where date conversion failed\n",
        "\n",
        "        # compute elapsed time (seconds) within each person-session\n",
        "        if compute_elapsed:\n",
        "            # Add check for empty groups which can cause errors\n",
        "            if not df.empty:\n",
        "                 df['elapsed_time'] = df.groupby(['person', 'session'], observed=True)['window_start'] \\\n",
        "                                       .transform(lambda x: (x - x.min()).dt.total_seconds())\n",
        "            else:\n",
        "                 df['elapsed_time'] = 0.0\n",
        "        else:\n",
        "            df['elapsed_time'] = 0.0\n",
        "\n",
        "        # sort to preserve temporal order per session\n",
        "        self.data = df.sort_values(['person', 'session', 'window_start']).reset_index(drop=True)\n",
        "\n",
        "        # Build session_map if not given\n",
        "        if session_map is None:\n",
        "            all_sessions = sorted(self.data['session'].unique())\n",
        "            self.session_map = {s: i for i, s in enumerate(all_sessions)}\n",
        "        else:\n",
        "            self.session_map = session_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # Modalities - handle potential missing keys gracefully\n",
        "        hr_mean = row.get('hr_mean', 0.0)\n",
        "        hr_std = row.get('hr_std', 0.0)\n",
        "        dur_mean = row.get('duration_mean', 0.0)\n",
        "        dur_std = row.get('duration_std', 0.0)\n",
        "        ax_mean = row.get('ax_mean', 0.0); ax_std = row.get('ax_std', 0.0)\n",
        "        ay_mean = row.get('ay_mean', 0.0); ay_std = row.get('ay_std', 0.0)\n",
        "        az_mean = row.get('az_mean', 0.0); az_std = row.get('az_std', 0.0)\n",
        "        eda_mean = row.get('eda_mean', 0.0); eda_std = row.get('eda_std', 0.0)\n",
        "        temp_mean = row.get('temp_mean', 0.0); temp_std = row.get('temp_std', 0.0)\n",
        "\n",
        "        HR   = torch.tensor([hr_mean, hr_std], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([dur_mean, dur_std], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([ax_mean, ax_std, ay_mean, ay_std, az_mean, az_std], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([eda_mean, eda_std], dtype=torch.float32)\n",
        "        Temp = torch.tensor([temp_mean, temp_std], dtype=torch.float32)\n",
        "\n",
        "        # Targets (regression)\n",
        "        phys  = torch.tensor([row.get('physicalFatigueScore', 0.0)], dtype=torch.float32)\n",
        "        ment  = torch.tensor([row.get('mentalFatigueScore', 0.0)], dtype=torch.float32)\n",
        "\n",
        "        # Session numeric feature\n",
        "        session_val = int(row.get('session', 0))\n",
        "        session_index = self.session_map.get(session_val, 0) # Default to index 0 if session not in map\n",
        "        session_feat = torch.tensor([float(session_index)], dtype=torch.float32)\n",
        "\n",
        "        # Elapsed time feature (seconds)\n",
        "        elapsed = torch.tensor([float(row.get('elapsed_time', 0.0))], dtype=torch.float32)\n",
        "\n",
        "        # Domain discrepancy placeholder\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)\n",
        "\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, session_feat, elapsed, (phys, ment)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    # Filter out None items potentially caused by errors in __getitem__\n",
        "    batch = [b for b in batch if b is not None]\n",
        "    if not batch:\n",
        "        # Return empty tensors or raise error if batch is empty after filtering\n",
        "        # Returning None signals DataLoader to skip this batch if batch_size > 0 and drop_last=False\n",
        "        return None\n",
        "\n",
        "    # Stack modality tensors: output is a list of 5 tensors each [B, feature_dim]\n",
        "    try:\n",
        "        modal_inputs = [torch.stack([b[0][i] for b in batch]) for i in range(5)]\n",
        "        domain_disc  = torch.stack([b[1] for b in batch])\n",
        "        session_feat = torch.stack([b[2] for b in batch])      # [B, 1]\n",
        "        elapsed_feat = torch.stack([b[3] for b in batch])      # [B, 1]\n",
        "        phys         = torch.stack([b[4][0] for b in batch])\n",
        "        ment         = torch.stack([b[4][1] for b in batch])\n",
        "        return modal_inputs, domain_disc, session_feat, elapsed_feat, (phys, ment)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during collation: {e}\")\n",
        "        # Return None to skip the batch if collation fails\n",
        "        return None\n",
        "\n",
        "# ========== Model ==========\n",
        "# (ModalityLSTM, DomainScalingMLP, CrossModalAttention, DomainAdaptiveLayer remain unchanged)\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        # small LSTM per modality (keeps same interface as original)\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # handle [B, feat] -> [B,1,feat]\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        # Ensure input tensor is contiguous\n",
        "        x = x.contiguous()\n",
        "        try:\n",
        "            out, _ = self.lstm(x)\n",
        "            return out[:, -1, :]  # last timestep\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ModalityLSTM forward: {e}\")\n",
        "            print(f\"Input shape: {x.shape}, Input dtype: {x.dtype}, Input device: {x.device}\")\n",
        "            # Depending on the error, you might want to return a zero tensor of expected shape\n",
        "            # Or re-raise the exception\n",
        "            raise e\n",
        "\n",
        "class DomainScalingMLP(nn.Module):\n",
        "    def __init__(self, input_dim=1, n_modalities=5, hidden=16):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, n_modalities),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, domain_disc):\n",
        "        return self.net(domain_disc)\n",
        "\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, n_modalities, hidden_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.key   = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.value = nn.Linear(hidden_dim, fusion_dim)\n",
        "        self.scaler = DomainScalingMLP(input_dim=1, n_modalities=n_modalities, hidden=16)\n",
        "\n",
        "    def forward(self, features, domain_disc, debug=False):\n",
        "        # features: list of [B, H] -> stack -> [B, M, H]\n",
        "        x = torch.stack(features, dim=1)  # [B, M, H]\n",
        "        Q, K, V = self.query(x), self.key(x), self.value(x)  # -> [B, M, F]\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (K.size(-1) ** 0.5)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Domain scaling (kept from original)\n",
        "        scaling = self.scaler(domain_disc).unsqueeze(1)  # [B,1,M]\n",
        "        attn = attn * scaling\n",
        "        # Add a small epsilon to prevent division by zero or NaN issues if all inputs are zero after scaling\n",
        "        attn = F.softmax(attn + 1e-9, dim=-1)\n",
        "\n",
        "\n",
        "        if debug and DEBUG_ATTENTION:\n",
        "            print(\"Domain scaling factors:\", scaling[0, 0].detach().cpu().numpy())\n",
        "            print(\"Attention weights:\", attn[0].detach().cpu().numpy())\n",
        "\n",
        "        fused_per_query = torch.matmul(attn, V)  # [B, M, F]\n",
        "        fused = fused_per_query.mean(dim=1)      # [B, F] (average across queries)\n",
        "        return fused\n",
        "\n",
        "\n",
        "class DomainAdaptiveLayer(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(dim, dim)\n",
        "        self.bn = nn.BatchNorm1d(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Need to handle potential batch size of 1 during evaluation\n",
        "        if x.size(0) > 1:\n",
        "            return self.bn(self.fc(x))\n",
        "        else:\n",
        "            # BatchNorm1d throws error for batch size 1, just pass through linear layer\n",
        "            return self.fc(x)\n",
        "\n",
        "\n",
        "class FMAL_Daf(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=64, fusion_dim=128,\n",
        "                 num_sessions=3, use_time_and_session=True):\n",
        "        super().__init__()\n",
        "        self.use_time_and_session = use_time_and_session\n",
        "\n",
        "        # modality LSTMs\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(d, lstm_hidden) for d in modalities_dim])\n",
        "\n",
        "        # attention fusion\n",
        "        self.attn_fusion = CrossModalAttention(len(modalities_dim), lstm_hidden, fusion_dim)\n",
        "\n",
        "        # domain-adaptive transformation\n",
        "        self.domain_adapt = DomainAdaptiveLayer(fusion_dim)\n",
        "\n",
        "        # session embedding\n",
        "        self.num_sessions = num_sessions # Store it\n",
        "        if num_sessions > 0:\n",
        "             self.session_emb = nn.Embedding(num_sessions, 8)\n",
        "        else:\n",
        "             # Handle case with 0 sessions if it can occur, maybe default to zeros?\n",
        "             self.session_emb = None # Or a dummy module\n",
        "\n",
        "        # elapsed time encoder\n",
        "        self.time_enc = nn.Sequential(\n",
        "            nn.Linear(1, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 8)\n",
        "        )\n",
        "\n",
        "        # global LSTM input dim calculation\n",
        "        global_input_dim = fusion_dim\n",
        "        if use_time_and_session and self.session_emb is not None:\n",
        "            global_input_dim += 16 # 8 for session, 8 for time\n",
        "\n",
        "        self.global_lstm = nn.LSTM(global_input_dim, 32, batch_first=True)\n",
        "\n",
        "        # regression heads\n",
        "        self.reg_phys = nn.Linear(32, 1)\n",
        "        self.reg_ment = nn.Linear(32, 1)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LSTM):\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "             nn.init.uniform_(m.weight, -0.1, 0.1)\n",
        "\n",
        "\n",
        "    def forward(self, modal_inputs, domain_disc, session_feat=None, elapsed_feat=None, debug=False):\n",
        "        feats = [m(inp) for m, inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = self.attn_fusion(feats, domain_disc, debug=debug)\n",
        "        adapted = self.domain_adapt(fused)\n",
        "\n",
        "        if self.use_time_and_session:\n",
        "            if session_feat is None or elapsed_feat is None:\n",
        "                raise ValueError(\"Need session_feat and elapsed_feat when use_time_and_session is True\")\n",
        "            if self.session_emb is None:\n",
        "                 raise ValueError(\"Session embedding is not initialized, likely num_sessions was 0.\")\n",
        "\n",
        "\n",
        "            # session_feat is already int-mapped\n",
        "            session_indices = session_feat.squeeze(-1).long()\n",
        "            # Clamp indices to be within the valid range for the embedding layer\n",
        "            session_indices = torch.clamp(session_indices, 0, self.num_sessions - 1)\n",
        "            session_emb = self.session_emb(session_indices)\n",
        "\n",
        "            time_emb = self.time_enc(elapsed_feat)\n",
        "            adapted = torch.cat([adapted, session_emb, time_emb], dim=1)\n",
        "\n",
        "        # Ensure input is contiguous before LSTM\n",
        "        adapted = adapted.contiguous()\n",
        "        glstm_out, _ = self.global_lstm(adapted.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_phys(feat), self.reg_ment(feat)\n",
        "\n",
        "\n",
        "# ========== Train / Eval ==========\n",
        "\n",
        "# --- MODIFICATION START ---\n",
        "# Helper function to evaluate model performance for one epoch (used during training)\n",
        "def evaluate_epoch(model, loader, device, crit_reg):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    regL = 0.0\n",
        "    all_phys_preds, all_phys_labels = [], []\n",
        "    all_ment_preds, all_ment_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in loader:\n",
        "             # Skip batch if collate function returned None\n",
        "             if batch_data is None:\n",
        "                 print(\"Skipping a batch due to collation error.\")\n",
        "                 continue\n",
        "             modals, dom, session_feat, elapsed_feat, (phys, ment) = batch_data\n",
        "\n",
        "             modals = [m.to(device) for m in modals]\n",
        "             dom = dom.to(device)\n",
        "             session_feat = session_feat.to(device)\n",
        "             elapsed_feat = elapsed_feat.to(device)\n",
        "             phys = phys.to(device)\n",
        "             ment = ment.to(device)\n",
        "\n",
        "             p_pred, m_pred = model(modals, dom, session_feat, elapsed_feat)\n",
        "             loss_r = crit_reg(p_pred, phys) + crit_reg(m_pred, ment) # Sum loss for eval\n",
        "             regL += loss_r.item()\n",
        "\n",
        "             all_phys_preds.extend(p_pred.squeeze().cpu().numpy())\n",
        "             all_phys_labels.extend(phys.squeeze().cpu().numpy())\n",
        "             all_ment_preds.extend(m_pred.squeeze().cpu().numpy())\n",
        "             all_ment_labels.extend(ment.squeeze().cpu().numpy())\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_regL = regL / len(loader) if len(loader) > 0 else float('nan')\n",
        "\n",
        "    # Metrics calculation (similar to evaluate_and_show_predictions)\n",
        "    all_phys_labels = pd.Series(all_phys_labels).fillna(0).values\n",
        "    all_phys_preds = pd.Series(all_phys_preds).fillna(0).values\n",
        "    all_ment_labels = pd.Series(all_ment_labels).fillna(0).values\n",
        "    all_ment_preds = pd.Series(all_ment_preds).fillna(0).values\n",
        "\n",
        "    if len(all_phys_labels) < 2:\n",
        "         rmse_phys, mae_phys, r2_phys = float('nan'), float('nan'), float('nan')\n",
        "    else:\n",
        "        rmse_phys = math.sqrt(mean_squared_error(all_phys_labels, all_phys_preds))\n",
        "        mae_phys  = mean_absolute_error(all_phys_labels, all_phys_preds)\n",
        "        r2_phys   = r2_score(all_phys_labels, all_phys_preds)\n",
        "\n",
        "    if len(all_ment_labels) < 2:\n",
        "        rmse_ment, mae_ment, r2_ment = float('nan'), float('nan'), float('nan')\n",
        "    else:\n",
        "        rmse_ment = math.sqrt(mean_squared_error(all_ment_labels, all_ment_preds))\n",
        "        mae_ment  = mean_absolute_error(all_ment_labels, all_ment_preds)\n",
        "        r2_ment   = r2_score(all_ment_labels, all_ment_preds)\n",
        "\n",
        "    metrics_dict = {\n",
        "        'loss': avg_regL,\n",
        "        'rmse_physical': rmse_phys,\n",
        "        'mae_physical': mae_phys,\n",
        "        'r2_physical': r2_phys,\n",
        "        'rmse_mental': rmse_ment,\n",
        "        'mae_mental': mae_ment,\n",
        "        'r2_mental': r2_ment\n",
        "    }\n",
        "    return metrics_dict\n",
        "# --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "# Modified train_model to collect metrics each epoch\n",
        "def train_model(model, train_loader, eval_loader, optim, crit_reg, epochs=10, device=torch.device(\"cpu\")):\n",
        "    # --- MODIFICATION START ---\n",
        "    # Initialize dictionary to store metrics history like the previous example\n",
        "    metrics_history = defaultdict(list)\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train() # Set model to training mode\n",
        "        tot_train_loss = 0.0\n",
        "        for i, batch_data in enumerate(train_loader):\n",
        "             # Skip batch if collate function returned None\n",
        "             if batch_data is None:\n",
        "                 print(f\"Skipping training batch {i} due to collation error.\")\n",
        "                 continue\n",
        "             modals, dom, session_feat, elapsed_feat, (phys, ment) = batch_data\n",
        "\n",
        "             modals = [m.to(device) for m in modals]\n",
        "             dom = dom.to(device)\n",
        "             session_feat = session_feat.to(device)\n",
        "             elapsed_feat = elapsed_feat.to(device)\n",
        "             phys = phys.to(device)\n",
        "             ment = ment.to(device)\n",
        "\n",
        "             optim.zero_grad()\n",
        "             debug_flag = (ep == 1 and i == 0 and DEBUG_ATTENTION) # Pass debug only if flag is True\n",
        "             try:\n",
        "                 p_pred, m_pred = model(modals, dom, session_feat, elapsed_feat, debug=debug_flag)\n",
        "                 loss = crit_reg(p_pred, phys) + 2*crit_reg(m_pred, ment) # Weight mental loss more\n",
        "                 # Check for NaN loss\n",
        "                 if torch.isnan(loss):\n",
        "                      print(f\"Warning: NaN loss detected at epoch {ep}, batch {i}. Skipping backward pass.\")\n",
        "                      # Optionally add more debugging here: print inputs, model parameters, etc.\n",
        "                      continue # Skip this batch update\n",
        "                 loss.backward()\n",
        "                 # Optional: Gradient clipping\n",
        "                 # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                 optim.step()\n",
        "                 tot_train_loss += loss.item()\n",
        "             except Exception as e:\n",
        "                 print(f\"Error during training forward/backward pass at epoch {ep}, batch {i}: {e}\")\n",
        "                 # Decide how to handle: skip batch, stop training, etc.\n",
        "                 continue # Skip batch on error\n",
        "\n",
        "\n",
        "        avg_train_loss = tot_train_loss / len(train_loader) if len(train_loader) > 0 else float('nan')\n",
        "        print(f\"[Epoch {ep}] Avg Training Loss: {avg_train_loss:.6f}\")\n",
        "\n",
        "        # --- MODIFICATION START ---\n",
        "        # Evaluate on the evaluation set (e.g., test set) after each epoch\n",
        "        epoch_metrics = evaluate_epoch(model, eval_loader, device, crit_reg)\n",
        "        print(f\"[Epoch {ep}] Evaluation Metrics:\")\n",
        "        pprint.pprint(epoch_metrics) # Print metrics for this epoch\n",
        "\n",
        "        # Append metrics to history with epoch number (round)\n",
        "        for key, value in epoch_metrics.items():\n",
        "            metrics_history[key].append((ep, value)) # Store as (epoch, value) tuple\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "    # --- MODIFICATION START ---\n",
        "    # Return the collected history\n",
        "    return dict(metrics_history) # Convert back to regular dict if preferred\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "# evaluate_and_show_predictions remains largely the same, but now it's for the *final* evaluation\n",
        "def evaluate_and_show_predictions(model, test_loader, device, crit_reg):\n",
        "    # This function now primarily serves to show final performance and sample predictions\n",
        "    # The metrics calculation part is duplicated from evaluate_epoch, could be refactored\n",
        "    model.eval()\n",
        "    regL = 0.0\n",
        "    all_phys_preds, all_phys_labels = [], []\n",
        "    all_ment_preds, all_ment_labels = [], []\n",
        "    first_batch_printed = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch_data in enumerate(test_loader):\n",
        "             if batch_data is None:\n",
        "                 print(\"Skipping a batch due to collation error during final evaluation.\")\n",
        "                 continue\n",
        "             modals, dom, session_feat, elapsed_feat, (phys, ment) = batch_data\n",
        "\n",
        "             modals = [m.to(device) for m in modals]\n",
        "             dom = dom.to(device)\n",
        "             session_feat = session_feat.to(device)\n",
        "             elapsed_feat = elapsed_feat.to(device)\n",
        "             phys = phys.to(device)\n",
        "             ment = ment.to(device)\n",
        "\n",
        "             p_pred, m_pred = model(modals, dom, session_feat, elapsed_feat)\n",
        "             loss_r = crit_reg(p_pred, phys) + crit_reg(m_pred, ment)\n",
        "             regL += loss_r.item()\n",
        "\n",
        "             # Ensure predictions/labels are detached and moved to CPU before extending list\n",
        "             all_phys_preds.extend(p_pred.squeeze().detach().cpu().numpy().tolist())\n",
        "             all_phys_labels.extend(phys.squeeze().detach().cpu().numpy().tolist())\n",
        "             all_ment_preds.extend(m_pred.squeeze().detach().cpu().numpy().tolist())\n",
        "             all_ment_labels.extend(ment.squeeze().detach().cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "             if not first_batch_printed:\n",
        "                 print(\"\\n=== Predictions vs Ground Truth (first test batch - FINAL EVAL) ===\")\n",
        "                 num_to_print = min(len(phys), 10)\n",
        "                 for j in range(num_to_print):\n",
        "                     # Check if predictions are single items or arrays before calling .item()\n",
        "                     p_val = p_pred[j].item() if p_pred[j].numel() == 1 else p_pred[j].detach().cpu().numpy()[0]\n",
        "                     m_val = m_pred[j].item() if m_pred[j].numel() == 1 else m_pred[j].detach().cpu().numpy()[0]\n",
        "                     phys_val = phys[j].item() if phys[j].numel() == 1 else phys[j].detach().cpu().numpy()[0]\n",
        "                     ment_val = ment[j].item() if ment[j].numel() == 1 else ment[j].detach().cpu().numpy()[0]\n",
        "                     print(f\"Sample {j}: True phys={phys_val:.3f}, True ment={ment_val:.3f} | \"\n",
        "                           f\"Pred phys={p_val:.3f}, Pred ment={m_val:.3f}\")\n",
        "                 first_batch_printed = True\n",
        "\n",
        "\n",
        "    avg_regL = regL / len(test_loader) if len(test_loader) > 0 else float('nan')\n",
        "\n",
        "    # Convert lists to numpy arrays for metric calculation\n",
        "    # Using pandas Series first helps handle potential variations in list content (e.g., scalar vs array)\n",
        "    all_phys_labels_np = pd.Series(all_phys_labels).fillna(0).values\n",
        "    all_phys_preds_np = pd.Series(all_phys_preds).fillna(0).values\n",
        "    all_ment_labels_np = pd.Series(all_ment_labels).fillna(0).values\n",
        "    all_ment_preds_np = pd.Series(all_ment_preds).fillna(0).values\n",
        "\n",
        "    # Metrics Calculation\n",
        "    if len(all_phys_labels_np) < 2:\n",
        "         rmse_phys, mae_phys, r2_phys = float('nan'), float('nan'), float('nan')\n",
        "    else:\n",
        "        # Ensure labels/preds are 1D arrays\n",
        "        all_phys_labels_np = all_phys_labels_np.ravel()\n",
        "        all_phys_preds_np = all_phys_preds_np.ravel()\n",
        "        rmse_phys = math.sqrt(mean_squared_error(all_phys_labels_np, all_phys_preds_np))\n",
        "        mae_phys  = mean_absolute_error(all_phys_labels_np, all_phys_preds_np)\n",
        "        r2_phys   = r2_score(all_phys_labels_np, all_phys_preds_np)\n",
        "\n",
        "\n",
        "    if len(all_ment_labels_np) < 2:\n",
        "        rmse_ment, mae_ment, r2_ment = float('nan'), float('nan'), float('nan')\n",
        "    else:\n",
        "        # Ensure labels/preds are 1D arrays\n",
        "        all_ment_labels_np = all_ment_labels_np.ravel()\n",
        "        all_ment_preds_np = all_ment_preds_np.ravel()\n",
        "        rmse_ment = math.sqrt(mean_squared_error(all_ment_labels_np, all_ment_preds_np))\n",
        "        mae_ment  = mean_absolute_error(all_ment_labels_np, all_ment_preds_np)\n",
        "        r2_ment   = r2_score(all_ment_labels_np, all_ment_preds_np)\n",
        "\n",
        "\n",
        "    # --- FINAL EVALUATION METRICS ---\n",
        "    final_metrics_dict = {\n",
        "        'loss': avg_regL,\n",
        "        'rmse_physical': rmse_phys,\n",
        "        'mae_physical': mae_phys,\n",
        "        'r2_physical': r2_phys,\n",
        "        'rmse_mental': rmse_ment,\n",
        "        'mae_mental': mae_ment,\n",
        "        'r2_mental': r2_ment\n",
        "    }\n",
        "\n",
        "    print(\"\\n=== Final Test Performance (after all epochs) ===\")\n",
        "    print(f\"Test Regression Loss (avg): {avg_regL:.6f}\")\n",
        "    print(f\"Physical Fatigue - RMSE: {rmse_phys:.6f}, MAE: {mae_phys:.6f}, R²: {r2_phys:.6f}\")\n",
        "    print(f\"Mental Fatigue   - RMSE: {rmse_ment:.6f}, MAE: {mae_ment:.6f}, R²: {r2_ment:.6f}\")\n",
        "\n",
        "    print(\"\\n=== Final Metrics Dictionary (after all epochs) ===\")\n",
        "    pprint.pprint(final_metrics_dict)\n",
        "    # This function now implicitly returns None, but primarily prints results\n",
        "\n",
        "\n",
        "# ========== Main ==========\n",
        "if __name__ == \"__main__\":\n",
        "    # replace with your CSV path - ENSURE THIS PATH IS CORRECT\n",
        "    csv_path = '/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized_interpolated.csv'\n",
        "    # csv_path = 'path/to/your/local/file.csv' # Example for local machine\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        print(f\"Successfully loaded data from {csv_path}. Shape: {df.shape}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: CSV file not found at {csv_path}\")\n",
        "        print(\"Please ensure the path is correct and the file exists.\")\n",
        "        exit()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # Data Cleaning and Preparation\n",
        "    required_cols = ['session', 'person', 'window_start', 'hr_mean', 'hr_std',\n",
        "                     'duration_mean', 'duration_std', 'ax_mean', 'ax_std',\n",
        "                     'ay_mean', 'ay_std', 'az_mean', 'az_std', 'eda_mean', 'eda_std',\n",
        "                     'temp_mean', 'temp_std', 'physicalFatigueScore', 'mentalFatigueScore']\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"Error: Missing required columns in CSV: {missing_cols}\")\n",
        "        exit()\n",
        "\n",
        "    # Convert columns to numeric where possible, coercing errors\n",
        "    for col in required_cols:\n",
        "         if col not in ['window_start']: # Skip date column for now\n",
        "              df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "\n",
        "    # Drop rows with NaN in features OR targets (more robust)\n",
        "    initial_len = len(df)\n",
        "    df.dropna(subset=required_cols, inplace=True)\n",
        "    if len(df) < initial_len:\n",
        "        print(f\"Dropped {initial_len - len(df)} rows containing NaN values in required columns.\")\n",
        "\n",
        "    if df.empty:\n",
        "         print(\"Error: DataFrame is empty after dropping NaN values. Cannot proceed.\")\n",
        "         exit()\n",
        "\n",
        "\n",
        "    # Compute session_map from potentially cleaned df\n",
        "    unique_sessions = sorted(df['session'].astype(int).unique())\n",
        "    if not unique_sessions:\n",
        "        print(\"Error: No valid sessions found in the data.\")\n",
        "        exit()\n",
        "    session_map = {s: i for i, s in enumerate(unique_sessions)}\n",
        "    num_sessions = len(session_map)\n",
        "    print(f\"Found {num_sessions} unique sessions.\")\n",
        "\n",
        "    # Simple 80/20 random split\n",
        "    shuffled = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    cut = int(0.8 * len(shuffled))\n",
        "    train_df = shuffled.iloc[:cut].reset_index(drop=True)\n",
        "    test_df  = shuffled.iloc[cut:].reset_index(drop=True)\n",
        "\n",
        "    print(f\"Training set size: {len(train_df)}\")\n",
        "    print(f\"Test set size: {len(test_df)}\")\n",
        "    if len(train_df) == 0 or len(test_df) == 0:\n",
        "        print(\"Error: Train or test set is empty after splitting.\")\n",
        "        exit()\n",
        "\n",
        "    train_dataset = FatigueSessionDataset(train_df, session_map=session_map, compute_elapsed=True)\n",
        "    test_dataset  = FatigueSessionDataset(test_df,  session_map=session_map, compute_elapsed=True)\n",
        "\n",
        "    # Dataloaders - set num_workers=0 for debugging, increase for performance later if possible\n",
        "    # Add drop_last=True to training loader to prevent potential batch size 1 issues with BatchNorm\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate, num_workers=0, drop_last=True)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Model Initialization\n",
        "    # Make sure num_sessions is correctly passed\n",
        "    if num_sessions == 0:\n",
        "         print(\"Error: Cannot initialize model with num_sessions=0.\")\n",
        "         exit()\n",
        "\n",
        "    model = FMAL_Daf([2,2,6,2,2], lstm_hidden=32, fusion_dim=64,\n",
        "                     num_sessions=num_sessions, use_time_and_session=True).to(device)\n",
        "\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    crit_reg = nn.MSELoss()\n",
        "\n",
        "    # --- MODIFICATION START ---\n",
        "    # Train the model and get the metrics history\n",
        "    # Pass test_loader as the evaluation loader for simplicity here\n",
        "    num_epochs = 47 # Set number of epochs\n",
        "    all_epoch_metrics = train_model(model, train_loader, test_loader, optim, crit_reg,\n",
        "                                    epochs=num_epochs, device=device)\n",
        "\n",
        "    # Print the final history dictionary after training completes\n",
        "    print(\"\\n=== Full Metrics History (All Epochs) ===\")\n",
        "    pprint.pprint(all_epoch_metrics)\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "    # --- FINAL EVALUATION ---\n",
        "    # Perform a final evaluation on the test set and print detailed results\n",
        "    # This call is somewhat redundant as the last epoch's metrics are in all_epoch_metrics,\n",
        "    # but it explicitly shows the final state and prints sample predictions.\n",
        "    print(\"\\n--- Running Final Evaluation on Test Set ---\")\n",
        "    evaluate_and_show_predictions(model, test_loader, device, crit_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XF1w4GSl0wD"
      },
      "source": [
        "#Centralized Model - Without Novelty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f02wciPul0g6",
        "outputId": "83f698a7-e49f-4897-b6b8-98a53e47c7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Avg Training Loss: 0.513347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 2] Avg Training Loss: 0.161110\n",
            "[Epoch 3] Avg Training Loss: 0.156698\n",
            "[Epoch 4] Avg Training Loss: 0.155157\n",
            "[Epoch 5] Avg Training Loss: 0.154061\n",
            "[Epoch 6] Avg Training Loss: 0.152778\n",
            "[Epoch 7] Avg Training Loss: 0.154899\n",
            "[Epoch 8] Avg Training Loss: 0.154858\n",
            "[Epoch 9] Avg Training Loss: 0.151549\n",
            "[Epoch 10] Avg Training Loss: 0.149262\n",
            "[Epoch 11] Avg Training Loss: 0.150932\n",
            "[Epoch 12] Avg Training Loss: 0.150330\n",
            "[Epoch 13] Avg Training Loss: 0.152497\n",
            "[Epoch 14] Avg Training Loss: 0.148133\n",
            "[Epoch 15] Avg Training Loss: 0.149063\n",
            "[Epoch 16] Avg Training Loss: 0.146970\n",
            "[Epoch 17] Avg Training Loss: 0.147941\n",
            "[Epoch 18] Avg Training Loss: 0.148184\n",
            "[Epoch 19] Avg Training Loss: 0.146879\n",
            "[Epoch 20] Avg Training Loss: 0.146558\n",
            "[Epoch 21] Avg Training Loss: 0.145402\n",
            "[Epoch 22] Avg Training Loss: 0.145739\n",
            "[Epoch 23] Avg Training Loss: 0.142945\n",
            "[Epoch 24] Avg Training Loss: 0.144016\n",
            "[Epoch 25] Avg Training Loss: 0.143619\n",
            "[Epoch 26] Avg Training Loss: 0.141361\n",
            "[Epoch 27] Avg Training Loss: 0.140122\n",
            "[Epoch 28] Avg Training Loss: 0.142886\n",
            "[Epoch 29] Avg Training Loss: 0.141957\n",
            "[Epoch 30] Avg Training Loss: 0.139382\n",
            "[Epoch 31] Avg Training Loss: 0.138060\n",
            "[Epoch 32] Avg Training Loss: 0.139107\n",
            "[Epoch 33] Avg Training Loss: 0.137976\n",
            "[Epoch 34] Avg Training Loss: 0.136618\n",
            "[Epoch 35] Avg Training Loss: 0.135993\n",
            "[Epoch 36] Avg Training Loss: 0.136891\n",
            "[Epoch 37] Avg Training Loss: 0.135483\n",
            "[Epoch 38] Avg Training Loss: 0.137769\n",
            "[Epoch 39] Avg Training Loss: 0.132292\n",
            "[Epoch 40] Avg Training Loss: 0.132129\n",
            "[Epoch 41] Avg Training Loss: 0.132714\n",
            "[Epoch 42] Avg Training Loss: 0.130349\n",
            "[Epoch 43] Avg Training Loss: 0.131351\n",
            "[Epoch 44] Avg Training Loss: 0.128653\n",
            "[Epoch 45] Avg Training Loss: 0.131946\n",
            "[Epoch 46] Avg Training Loss: 0.129705\n",
            "[Epoch 47] Avg Training Loss: 0.128395\n",
            "=== Predictions vs Ground Truth (first batch) ===\n",
            "Sample 0: True phys=0.187, True ment=0.183 | Pred phys=0.221, Pred ment=0.117\n",
            "Sample 1: True phys=0.187, True ment=0.183 | Pred phys=0.213, Pred ment=0.315\n",
            "Sample 2: True phys=0.187, True ment=0.183 | Pred phys=0.124, Pred ment=0.228\n",
            "Sample 3: True phys=0.188, True ment=0.183 | Pred phys=0.152, Pred ment=0.326\n",
            "Sample 4: True phys=0.200, True ment=0.180 | Pred phys=0.161, Pred ment=0.321\n",
            "Sample 5: True phys=0.208, True ment=0.177 | Pred phys=0.158, Pred ment=0.291\n",
            "Sample 6: True phys=0.261, True ment=0.161 | Pred phys=0.199, Pred ment=0.340\n",
            "Sample 7: True phys=0.265, True ment=0.160 | Pred phys=0.202, Pred ment=0.345\n",
            "Sample 8: True phys=0.269, True ment=0.158 | Pred phys=0.204, Pred ment=0.343\n",
            "Sample 9: True phys=0.278, True ment=0.156 | Pred phys=0.206, Pred ment=0.349\n",
            "Sample 10: True phys=0.310, True ment=0.146 | Pred phys=0.198, Pred ment=0.344\n",
            "Sample 11: True phys=0.308, True ment=0.178 | Pred phys=0.202, Pred ment=0.353\n",
            "Sample 12: True phys=0.294, True ment=0.299 | Pred phys=0.219, Pred ment=0.379\n",
            "Sample 13: True phys=0.283, True ment=0.395 | Pred phys=0.245, Pred ment=0.403\n",
            "Sample 14: True phys=0.268, True ment=0.517 | Pred phys=0.281, Pred ment=0.433\n",
            "Sample 15: True phys=0.267, True ment=0.525 | Pred phys=0.280, Pred ment=0.433\n",
            "Sample 16: True phys=0.262, True ment=0.569 | Pred phys=0.285, Pred ment=0.436\n",
            "Sample 17: True phys=0.259, True ment=0.594 | Pred phys=0.299, Pred ment=0.448\n",
            "Sample 18: True phys=0.198, True ment=0.433 | Pred phys=0.223, Pred ment=0.153\n",
            "Sample 19: True phys=0.198, True ment=0.433 | Pred phys=0.129, Pred ment=0.214\n",
            "Sample 20: True phys=0.198, True ment=0.433 | Pred phys=0.116, Pred ment=0.236\n",
            "Sample 21: True phys=0.248, True ment=0.450 | Pred phys=0.132, Pred ment=0.245\n",
            "Sample 22: True phys=0.306, True ment=0.470 | Pred phys=0.149, Pred ment=0.256\n",
            "Sample 23: True phys=0.403, True ment=0.503 | Pred phys=0.167, Pred ment=0.269\n",
            "Sample 24: True phys=0.461, True ment=0.522 | Pred phys=0.194, Pred ment=0.277\n",
            "Sample 25: True phys=0.520, True ment=0.542 | Pred phys=0.280, Pred ment=0.360\n",
            "Sample 26: True phys=0.714, True ment=0.608 | Pred phys=0.440, Pred ment=0.442\n",
            "Sample 27: True phys=0.765, True ment=0.625 | Pred phys=0.411, Pred ment=0.459\n",
            "Sample 28: True phys=0.700, True ment=0.591 | Pred phys=0.314, Pred ment=0.400\n",
            "Sample 29: True phys=0.690, True ment=0.586 | Pred phys=0.322, Pred ment=0.431\n",
            "Sample 30: True phys=0.634, True ment=0.557 | Pred phys=0.293, Pred ment=0.418\n",
            "Sample 31: True phys=0.625, True ment=0.552 | Pred phys=0.289, Pred ment=0.416\n",
            "\n",
            "=== Final Test Performance ===\n",
            "Test Regression Loss (sum): 0.003317\n",
            "Physical Fatigue - RMSE: 0.177671, MAE: 0.130585, R²: 0.028643\n",
            "Mental Fatigue  - RMSE: 0.167750, MAE: 0.157128, R²: 0.097301\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# =========================\n",
        "# Dataset (unchanged)\n",
        "# =========================\n",
        "class FatigueSessionDataset(Dataset):\n",
        "    def __init__(self, df, session_map=None, compute_elapsed=True):\n",
        "        df = df.copy()\n",
        "        df['session'] = df['session'].astype(int)\n",
        "        df['person'] = df['person'].astype(int)\n",
        "        df['window_start'] = pd.to_datetime(df['window_start'])\n",
        "        if compute_elapsed:\n",
        "            df['elapsed_time'] = df.groupby(['person', 'session'])['window_start'] \\\n",
        "                                  .transform(lambda x: (x - x.min()).dt.total_seconds())\n",
        "        else:\n",
        "            df['elapsed_time'] = 0.0\n",
        "        self.data = df.sort_values(['person', 'session', 'window_start']).reset_index(drop=True)\n",
        "        if session_map is None:\n",
        "            all_sessions = sorted(self.data['session'].unique())\n",
        "            self.session_map = {s: i for i, s in enumerate(all_sessions)}\n",
        "        else:\n",
        "            self.session_map = session_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        HR   = torch.tensor([row['hr_mean'], row['hr_std']], dtype=torch.float32)\n",
        "        IBI  = torch.tensor([row['duration_mean'], row['duration_std']], dtype=torch.float32)\n",
        "        ACC  = torch.tensor([row['ax_mean'], row['ax_std'], row['ay_mean'], row['ay_std'], row['az_mean'], row['az_std']], dtype=torch.float32)\n",
        "        EDA  = torch.tensor([row['eda_mean'], row['eda_std']], dtype=torch.float32)\n",
        "        Temp = torch.tensor([row['temp_mean'], row['temp_std']], dtype=torch.float32)\n",
        "\n",
        "        phys  = torch.tensor([row['physicalFatigueScore']], dtype=torch.float32)\n",
        "        ment  = torch.tensor([row['mentalFatigueScore']], dtype=torch.float32)\n",
        "        session_index = self.session_map[int(row['session'])]\n",
        "        session_feat = torch.tensor([float(session_index)], dtype=torch.float32)\n",
        "        elapsed = torch.tensor([float(row['elapsed_time'])], dtype=torch.float32)\n",
        "        domain_disc = torch.tensor([0.0], dtype=torch.float32)  # placeholder\n",
        "        return [HR, IBI, ACC, EDA, Temp], domain_disc, session_feat, elapsed, (phys, ment)\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    modal_inputs = [torch.stack([b[0][i] for b in batch]) for i in range(5)]\n",
        "    domain_disc  = torch.stack([b[1] for b in batch])\n",
        "    session_feat = torch.stack([b[2] for b in batch])\n",
        "    elapsed_feat = torch.stack([b[3] for b in batch])\n",
        "    phys         = torch.stack([b[4][0] for b in batch])\n",
        "    ment         = torch.stack([b[4][1] for b in batch])\n",
        "    return modal_inputs, domain_disc, session_feat, elapsed_feat, (phys, ment)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Model\n",
        "# =========================\n",
        "class ModalityLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=32):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.lstm(x)\n",
        "        return out[:, -1, :]\n",
        "\n",
        "\n",
        "class BaselineFMAL(nn.Module):\n",
        "    def __init__(self, modalities_dim, lstm_hidden=32, use_time_and_session=True, num_sessions=3):\n",
        "        super().__init__()\n",
        "        self.use_time_and_session = use_time_and_session\n",
        "        self.modality_lstms = nn.ModuleList([ModalityLSTM(d, lstm_hidden) for d in modalities_dim])\n",
        "\n",
        "        # session embedding\n",
        "        self.session_emb = nn.Embedding(num_sessions, 8)\n",
        "\n",
        "        # elapsed time encoder\n",
        "        self.time_enc = nn.Sequential(\n",
        "            nn.Linear(1, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 8)\n",
        "        )\n",
        "\n",
        "        # global LSTM input = concat of modality LSTMs + session + time\n",
        "        global_input_dim = lstm_hidden * len(modalities_dim)\n",
        "        if use_time_and_session:\n",
        "            global_input_dim += 16\n",
        "        self.global_lstm = nn.LSTM(global_input_dim, 32, batch_first=True)\n",
        "\n",
        "        # regression heads\n",
        "        self.reg_phys = nn.Linear(32, 1)\n",
        "        self.reg_ment = nn.Linear(32, 1)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weights(m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LSTM):\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    nn.init.xavier_uniform_(param)\n",
        "                elif 'bias' in name:\n",
        "                    nn.init.zeros_(param)\n",
        "\n",
        "    def forward(self, modal_inputs, domain_disc=None, session_feat=None, elapsed_feat=None, debug=False):\n",
        "        feats = [m(inp) for m, inp in zip(self.modality_lstms, modal_inputs)]\n",
        "        fused = torch.cat(feats, dim=1)\n",
        "\n",
        "        if self.use_time_and_session:\n",
        "            session_emb = self.session_emb(session_feat.squeeze(-1).long())\n",
        "            time_emb = self.time_enc(elapsed_feat)\n",
        "            fused = torch.cat([fused, session_emb, time_emb], dim=1)\n",
        "\n",
        "        glstm_out, _ = self.global_lstm(fused.unsqueeze(1))\n",
        "        feat = glstm_out[:, -1, :]\n",
        "        return self.reg_phys(feat), self.reg_ment(feat)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Training / Evaluation (unchanged)\n",
        "# =========================\n",
        "def train_model(model, loader, optim, crit_reg, epochs=10, device=torch.device(\"cpu\")):\n",
        "    for ep in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        tot = 0.0\n",
        "        for modals, dom, session_feat, elapsed_feat, (phys, ment) in loader:\n",
        "            modals = [m.to(device) for m in modals]\n",
        "            session_feat = session_feat.to(device)\n",
        "            elapsed_feat = elapsed_feat.to(device)\n",
        "            phys = phys.to(device)\n",
        "            ment = ment.to(device)\n",
        "\n",
        "            optim.zero_grad()\n",
        "            p_pred, m_pred = model(modals, session_feat=session_feat, elapsed_feat=elapsed_feat)\n",
        "            loss = crit_reg(p_pred, phys) + 2*crit_reg(m_pred, ment)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            tot += loss.item()\n",
        "        avg_loss = tot / len(loader) if len(loader) > 0 else float('nan')\n",
        "        print(f\"[Epoch {ep}] Avg Training Loss: {avg_loss:.6f}\")\n",
        "\n",
        "\n",
        "def evaluate_and_show_predictions(model, test_loader, device, crit_reg):\n",
        "    model.eval()\n",
        "    regL = 0.0\n",
        "    all_phys_preds, all_phys_labels = [], []\n",
        "    all_ment_preds, all_ment_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for modals, dom, session_feat, elapsed_feat, (phys, ment) in test_loader:\n",
        "            modals = [m.to(device) for m in modals]\n",
        "            session_feat = session_feat.to(device)\n",
        "            elapsed_feat = elapsed_feat.to(device)\n",
        "            phys = phys.to(device)\n",
        "            ment = ment.to(device)\n",
        "\n",
        "            p_pred, m_pred = model(modals, session_feat=session_feat, elapsed_feat=elapsed_feat)\n",
        "            loss_r = crit_reg(p_pred, phys) + crit_reg(m_pred, ment)\n",
        "            regL += loss_r.item()\n",
        "\n",
        "            all_phys_preds.extend(p_pred.squeeze().cpu().numpy())\n",
        "            all_phys_labels.extend(phys.squeeze().cpu().numpy())\n",
        "            all_ment_preds.extend(m_pred.squeeze().cpu().numpy())\n",
        "            all_ment_labels.extend(ment.squeeze().cpu().numpy())\n",
        "\n",
        "            # print first batch only\n",
        "            print(\"=== Predictions vs Ground Truth (first batch) ===\")\n",
        "            for i in range(len(phys)):\n",
        "                print(f\"Sample {i}: True phys={phys[i].item():.3f}, True ment={ment[i].item():.3f} | \"\n",
        "                      f\"Pred phys={p_pred[i].item():.3f}, Pred ment={m_pred[i].item():.3f}\")\n",
        "            break\n",
        "\n",
        "    rmse_phys = math.sqrt(mean_squared_error(all_phys_labels, all_phys_preds))\n",
        "    mae_phys  = mean_absolute_error(all_phys_labels, all_phys_preds)\n",
        "    r2_phys   = r2_score(all_phys_labels, all_phys_preds)\n",
        "    rmse_ment = math.sqrt(mean_squared_error(all_ment_labels, all_ment_preds))\n",
        "    mae_ment  = mean_absolute_error(all_ment_labels, all_ment_preds)\n",
        "    r2_ment   = r2_score(all_ment_labels, all_ment_preds)\n",
        "\n",
        "    print(\"\\n=== Final Test Performance ===\")\n",
        "    print(f\"Test Regression Loss (sum): {regL/len(test_loader):.6f}\")\n",
        "    print(f\"Physical Fatigue - RMSE: {rmse_phys:.6f}, MAE: {mae_phys:.6f}, R²: {r2_phys:.6f}\")\n",
        "    print(f\"Mental Fatigue  - RMSE: {rmse_ment:.6f}, MAE: {mae_ment:.6f}, R²: {r2_ment:.6f}\")\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    csv_path = '/content/drive/MyDrive/Fatigue_Set/final_feature_label_dataset_normalized_interpolated.csv'\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    session_map = {s: i for i, s in enumerate(sorted(df['session'].astype(int).unique()))}\n",
        "    shuffled = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "    cut = int(0.8 * len(shuffled))\n",
        "    train_df = shuffled.iloc[:cut].reset_index(drop=True)\n",
        "    test_df  = shuffled.iloc[cut:].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = FatigueSessionDataset(train_df, session_map=session_map, compute_elapsed=True)\n",
        "    test_dataset  = FatigueSessionDataset(test_df,  session_map=session_map, compute_elapsed=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BaselineFMAL([2,2,6,2,2], lstm_hidden=32, use_time_and_session=True, num_sessions=len(session_map)).to(device)\n",
        "\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "    crit_reg = nn.MSELoss()\n",
        "\n",
        "    train_model(model, train_loader, optim, crit_reg, epochs=47, device=device)\n",
        "    evaluate_and_show_predictions(model, test_loader, device, crit_reg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH11iFAQzxKs"
      },
      "source": [
        "Training and Testing Data heads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss9JACVCb-XG"
      },
      "outputs": [],
      "source": [
        "# 🔎 Show dataset splits\n",
        "print(\"\\n=== Training Dataset Head ===\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\n=== Testing Dataset Head ===\")\n",
        "print(test_df.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}